{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXmVarw2kFl99/n0phqKdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag314/EDA/blob/main/Useful_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas basics\n",
        "Delete rows with NAN values\n",
        ">1.axis: Default – 0\n",
        "0, or ‘index’ : Drop rows which contain NaN values.\n",
        "1, or ‘columns’ : Drop columns which contain NaN value.\n",
        ">2.how: Default – ‘any’\n",
        "‘any’ : Drop rows / columns which contain any NaN values.\n",
        "‘all’ : Drop rows / columns which contain all NaN values.\n",
        ">3.thresh (int): Optional\n",
        "Delete rows/columns which contains less than minimun thresh number of non-NaN values.\n",
        ">4.inplace (bool): Default- False\n",
        "If True, modifies the calling dataframe object\n",
        "\n",
        "\n",
        "Returns\n",
        "\n",
        "If inplace==True, the return None, else returns a new dataframe by deleting the rows/columns based on NaN values\n",
        "you must add inplace = True argument, if you want the dataframe to be actually updated."
      ],
      "metadata": {
        "id": "QP5HaKbEI5BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rename column\n",
        "cci.rename(columns={'Value':'cci'},inplace=True)"
      ],
      "metadata": {
        "id": "vv3xjob5CTw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVJAF2xZIvHK"
      },
      "outputs": [],
      "source": [
        "#Pandas delete rows with NAN values\n",
        "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows which contain all NaN values\n",
        "df = df.dropna(axis=0, how='all')\n",
        "df.dropna(axis=1)# Drop columns that contain any NaN value.\n",
        "df.dropna(thresh=3,axis=1)# Drop any column that have less than 3 NaN values\n",
        "df.dropna(axis=column,how='all')# Dron columns that contain ALL NaN values."
      ],
      "metadata": {
        "id": "eEu26wPHK4fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#If you want to select rows with at least one NaN value, then you could use isna + any on axis=1:\n",
        "\n",
        "df[df.isna().any(axis=1)]\n",
        "#If you want to select rows with a certain number of NaN values, then you could use isna + sum on axis=1 + gt. For example, the following will fetch rows with at least 2 NaN values:\n",
        "\n",
        "df[df.isna().sum(axis=1)>1]\n",
        "\n",
        "# Select not NaN values\n",
        "df[df.notna().all(axis=1)]"
      ],
      "metadata": {
        "id": "lpwrXpCTM400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "WMhEl2-hxt9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print all columns names\n",
        "print(df.columns)\n",
        "#Print all columns types\n",
        "print(df.dtypes)\n",
        "# Select specific data types \n",
        "only_ints = df.select_dtypes(include=['int'])\n",
        "print(only_ints.columns)\n",
        "#rename dataset columns\n",
        "data.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n"
      ],
      "metadata": {
        "id": "1HLUQyzP768W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For a dataframe df, one can use any of the following: Number of rows in dataframe\n",
        "df.shape\n",
        "len(df.index)\n",
        "len(df.comlumns)# To find out number of columns\n",
        "df.shape[0]\n",
        "df[df.columns[0]].count() (== number of non-NaN values in first column)\n",
        "count_row = df.shape[0]  # Gives number of rows\n",
        "count_col = df.shape[1]  # Gives number of columns\n",
        "r, c = df.shape"
      ],
      "metadata": {
        "id": "s8OBL57nReev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas one hot encodding\n",
        "one_hot_encoded_data = pd.get_dummies(data, columns = ['Remarks', 'Gender'])"
      ],
      "metadata": {
        "id": "1cZGVbS6MTVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete rows with a condition.\n",
        "#Delete all rows if a row contains 3 missing values\n",
        "df.drop(df[df.isna().sum(axis=1)==3].index,inplace=True)\n",
        "\n",
        "#To remove all rows where column 'score' is < 50 and > 20\n",
        "#The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses.\n",
        "df = df.drop(df[(df.score < 50) & (df.score > 20)].index)\n",
        "\n",
        "# Drop record with specified values. Drop record where area >4000 and price , 210000\n",
        "df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<210000)].index)"
      ],
      "metadata": {
        "id": "r455qNCgaQT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NN output probability to boolean values.\n",
        "test2.loc[test2['Trans'] < 0.5, 'Transported'] = 'False' \n",
        "test2.loc[test2['Trans'] >= 0.5, 'Transported'] = 'True' \n",
        "test2.drop('Trans',axis=1,inplace=True)\n",
        "\n",
        "#Another way is to use replace function\n",
        "survivors_df['Survived'].replace([0], 'Not Survived', inplace = True)\n",
        "survivors_df['Survived'].replace([1], 'Survived', inplace = True)"
      ],
      "metadata": {
        "id": "B925e_ZhNz63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To find out unique values within columns\n",
        "x.iloc[:,3].unique() # All unique values in column 3\n",
        "temp.Pclass.unique()"
      ],
      "metadata": {
        "id": "_DxN-opXrWdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert numeric values to float\n",
        "numeric_list = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_list] = df[numeric_list].astype(np.float32)\n",
        "# Convert object type to numeric\n",
        "df[\"Total_Charges\"]=pd.to_numeric(df[\"Total_Charges\"])"
      ],
      "metadata": {
        "id": "SmxA_Q4j7jqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set floating point option to display 4 decimals.\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
      ],
      "metadata": {
        "id": "bnGqncNjtTtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many data is type of object\n",
        "\n",
        "df_data.select_dtypes(['object']).head()\n",
        "\n",
        "#Get all string columns \n",
        "string_cols = [f for f in train.columns if train[f].dtype == object]"
      ],
      "metadata": {
        "id": "dXICP9Ys3UQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the frequency of dataframe columns\n",
        "\n",
        "df1 = df['Courses'].value_counts()\n",
        "df1 = df.Courses.value_counts\n",
        "df1 = df.groupby('Courses').count()\n",
        "df1 = df.index.value_counts()"
      ],
      "metadata": {
        "id": "LF1rrZC4OCZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform all object and boolean column to integer\n",
        "for colname in x_train.select_dtypes(['object','bool']).columns:\n",
        "  x_train[colname]=LabelEncoder().fit_transform(x_train[colname])\n"
      ],
      "metadata": {
        "id": "Ar2vqdqa0_yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop whole column\n",
        "train=train.drop(['Age'].index,inplace=True)"
      ],
      "metadata": {
        "id": "4J-e26q5q5al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LEts check correlation matric to find out which features are important in prediction survival\n",
        "\n",
        "corrMatrix = train.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kFFbttYxvg0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the correlation between numerical variable with sale price\n",
        "corr=housepricesdata.corr()[\"SalePrice\"]\n",
        "\n",
        "# Sort the correlation values\n",
        "# Use [::-1] to sort it in descending manner\n",
        "# Use [::+1] to sort it in ascending manner\n",
        "corr[np.argsort(corr)[::-1]]"
      ],
      "metadata": {
        "id": "jPtwlwScriEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SalePrice         1.000000\n",
        "GrLivArea         0.708624\n",
        "GarageCars        0.640409\n",
        "GarageArea        0.623431\n",
        "TotalBsmtSF       0.613581\n",
        "1stFlrSF          0.605852\n",
        "FullBath          0.560664\n",
        "TotRmsAbvGrd      0.533723\n",
        "YearBuilt         0.522897\n",
        "YearRemodAdd      0.507101\n",
        "MasVnrArea        0.472614\n",
        "Fireplaces        0.466929\n",
        "BsmtFinSF1        0.386420\n",
        "LotFrontage       0.334771\n",
        "WoodDeckSF        0.324413\n",
        "2ndFlrSF          0.319334\n",
        "OpenPorchSF       0.315856\n",
        "HalfBath          0.284108\n",
        "LotArea           0.263843\n",
        "GarageYrBlt       0.261366\n",
        "BsmtFullBath      0.227122\n",
        "LotShape_IR1      0.223284\n",
        "BsmtUnfSF         0.214479\n",
        "BedroomAbvGr      0.168213\n",
        "LotShape_IR2      0.126096\n",
        "ScreenPorch       0.111447\n",
        "PoolArea          0.092404\n",
        "MoSold            0.046432\n",
        "3SsnPorch         0.044584\n",
        "LotShape_IR3      0.036720\n",
        "BsmtFinSF2       -0.011378\n",
        "BsmtHalfBath     -0.016844\n",
        "MiscVal          -0.021190\n",
        "LowQualFinSF     -0.025606\n",
        "YrSold           -0.028923\n",
        "EnclosedPorch    -0.128578\n",
        "KitchenAbvGr     -0.135907\n",
        "LotShape_Reg     -0.267672\n",
        "remodelled_age   -0.507101\n",
        "building_age     -0.522897\n",
        "Name: SalePrice, dtype: float64"
      ],
      "metadata": {
        "id": "yixQx-AVrrSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We check the correlation of each of our feature variables with the target variable\n",
        "df_creditcarddata.drop(['default.payment.next.month'], \\\n",
        "     axis = 1).corrwith(df_creditcarddata['default.payment.next.month']).\\\n",
        "     plot.bar(figsize=(20,10), \\\n",
        "     title = 'Correlation with Response variable', \\\n",
        "     fontsize = 15, rot = 45, grid = True)\n"
      ],
      "metadata": {
        "id": "hdQg5BRH7Ium"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross tab frequency table\n",
        "train.groupby(['Fare', 'Survived'])['Fare'].count().unstack()"
      ],
      "metadata": {
        "id": "kbkpZjDhyC45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of min max scaler in data which is not normal and has a range of values such as pixel values\n",
        "# Use of standard scaler when data is normally distributed.\n",
        "from sklearn.preprocessing import MinMaxScale\n",
        "scaler=MinMaxScaler()\n",
        "train['Fare']=scaler.fit_transform(train[['Fare']])"
      ],
      "metadata": {
        "id": "W7p2mPlb4Tto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to find median\n",
        "median = df['Age'].describe()[5]\n",
        "\n",
        "#If you get your output in scientific notation, you can change to view it in standard form instead by\n",
        "# executing the following command: pd.options.display.float_format = ‘{:.2f}’.format"
      ],
      "metadata": {
        "id": "FJBM-lcRDarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values with mode for fare in test data\n",
        "\n",
        "mode=train['Embarked'].value_counts().index[0]\n",
        "train['Embarked']=train['Embarked'].fillna(mode)\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "J0IPuN0H6nrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw barplot with cross tab freq values.\n",
        "ax=sns.countplot(data=temp,x='Pclass',hue='Survived')\n",
        "ax.bar_label(ax.containers[0])\n",
        "ax.bar_label(ax.containers[1])\n",
        "plt.legend(title='Survived or not',loc='upper left',labels=['No','Yes'])"
      ],
      "metadata": {
        "id": "Smai-73RpsDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to categories categories\n",
        "  data['Age']=pd.cut(data['Age'],bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])"
      ],
      "metadata": {
        "id": "TorY3W_9yqAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics from history object\n",
        "\n",
        "train_loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "train_accuracy=history.history['accuracy']\n",
        "val_accuracy=history.history['val_accuracy']\n",
        "epochs=range(len(train_accuracy))\n",
        "#plot loss\n",
        "plt.plot(epochs,train_loss,color='b',label='Train loss')\n",
        "plt.plot(epochs,val_loss,color='r',label='Validation loss')\n",
        "#plot accuracy\n",
        "plt.plot(epochs,train_accuracy,color='b',label='Train accuracy')\n",
        "plt.plot(epochs,val_accuracy,color='r',label='Validation accuracy')"
      ],
      "metadata": {
        "id": "gRlNb_aN-3PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGB model \n",
        "xgb_model=xgb.XGBClassifier(objective='binary:logistic',random_state=42)\n",
        "\n",
        "xgb_model.fit(tr_x,tr_y)\n",
        "y_pred=xgb_model.predict(cv_x)\n",
        "print(confusion_matrix(cv_y,y_pred))"
      ],
      "metadata": {
        "id": "L0qS_ZY6U1Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the feature importances \n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4R04_ZjJqQTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list comprehension Square all values in list x\n",
        "#num**2 for num in x\n",
        "\n",
        "# lambda function to square all values \n",
        "list(map(lambda x: x**2,range(1,6)))\n",
        "\n",
        "#argmax() and argmin() are used to find index location of max and min values in numpy array."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-hjNTPlnLDC",
        "outputId": "6ef411cb-bd40-4510-bade-42f6e6cc6b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "arr=np.arange(0,5)\n",
        "arr\n",
        "arr/arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgNIAKCx0BzG",
        "outputId": "76a22084-1c38-472b-d197-c6ea99e0e5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  1.,  1.,  1.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting index with col_name\n",
        "df.set_index('col_name',inplace=True)\n",
        "\n",
        "# Reseting index of the dataframe to numerical index(which is default index)\n",
        "\n",
        "df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "bngYLJTwhBoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict={'A':[1,2 ,np.nan,4,np.nan],\n",
        "           'B':[np.nan, np.nan,np.nan,np.nan,np.nan],\n",
        "           'C':[11,12,13,14,15],\n",
        "           'D':[16,np.nan,18,19,20]}\n",
        "df=pd.DataFrame(data_dict)\n",
        "df.fillna(method='ffill')# Use the last valid observation to fill the gap\n",
        "df.fillna(method='bfill')#Use next valid observation to fill the gap\n"
      ],
      "metadata": {
        "id": "3Dtd-xsMdt1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge datasets joins\n",
        "pd.merge(df1,df2,how='inner',on=['key'])\n",
        "pd.merge(df1,df2,how='outer',on=['key'])\n",
        "pd.merge(df1,df2,how='left',on=['key'])\n",
        "pd.merge(df1,df2,how='right',on=['key'])\n",
        "\n",
        "#Stack datasets\n",
        "pd.concat([df1],[df2])"
      ],
      "metadata": {
        "id": "fEhroj9hnTxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two features with highest chi-squared statistics are selected\n",
        "#For selecting categorical features .Change k for number of features.\n",
        "chi2_features = SelectKBest(chi2, k = 2)\n",
        "X_kbest_features = chi2_features.fit_transform(X, y)"
      ],
      "metadata": {
        "id": "Ro7o8yEKIMTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print importnant features in the model\n",
        "\n",
        "model=ExtraTreesClassifier()\n",
        "model.fit(x,y)\n",
        "print(model.feature_importances_)\n",
        "feature_importances=pd.Series(model.feature_importanes_,index=x.columns)\n",
        "feature_importances.nlargest(10).plot(kind='barh')# Draw horizontal bar graph of 10 most important features.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8SshKKVcYkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The correlation matrix shows only linear dependence. If we plot a rolling mean of the target probability for every feature, we'll see nonlinear dependences as well:\n",
        "# If we get non linear dependencies then \n",
        "ax.scatter(temp[f], temp.state.rolling(15000, center=True).mean(), s=2)# Plot rolling mean of 15000 values against feature ."
      ],
      "metadata": {
        "id": "ALF0SR2SwHSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use set_index and reset_index to find out how many family memebers from family name\n",
        "test_df['FamilyId'] = test_df['PassengerId'].str.split(\"_\", n=2, expand=True)[0]\n",
        "test_df['Family Name'] = test_df['Name'].str.split(' ', n=2, expand=True)[1]\n",
        "test_df = test_df.set_index(['FamilyId','Family Name'])# Setting the index with FamilyId and Family Name witll set index as an example below\n",
        "test_df['Family Size'] = 1 # Default family size\n",
        "​\n",
        "for i in range(test_df.shape[0]):\n",
        "    fam_size = test_df.loc[test_df.index[i],:].shape[0]# this gives list of (number of family members, total columns in test datasets)--> shape[0] will give total familly members\n",
        "    test_df.loc[test_df.index[i],'Family Size'] = fam_size\n",
        "​\n",
        "test_df=test_df.reset_index()# Reset index to default"
      ],
      "metadata": {
        "id": "Vu94XFwAa1Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering dataset\n",
        "#Check if countries are in column \n",
        "\n",
        "countries=[\"United States\",\"India\",\"United Kingdon\",\"Germany\",\"Canada\"]\n",
        "filt=df['Country'].isin(countries)\n",
        "df.loc[filt,'Country']"
      ],
      "metadata": {
        "id": "fnVAG42d1e6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find sum across columns \n",
        "df[['a','b','d']].agg('sum', axis=1)\n",
        "#The advantage of agg is that you can use multiple aggregation functions:\n",
        "\n",
        "df[['a','b','d']].agg(['sum', 'prod', 'min', 'max'], axis=1)"
      ],
      "metadata": {
        "id": "fnzCqO_yD3RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering based on if else condition. Create/update column based on existing column conditions.\n",
        "df['Interest']=[i*0.02 if i<20 else i*0.04 for i in df.debts]"
      ],
      "metadata": {
        "id": "RhsrOuyQRW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To achieve a compact visualization of the distribution that retains histogram-like characteristics, Hintze and Nelson (1998) developed the violin plot (c).\n",
        "#Violin plot is created by generating a density or distribution of the data and its mirror image. In the above figure we can see the violin plot, where we can\n",
        " #now see the many distinct peaks in citric acid distribution. The lower quartile, median, and upper quartile can be added to a violin plot to also consider \n",
        "# this information in the overall assessment of the distribution.\n",
        " fig = go.Figure(px.violin(df, y = 'sulphates', title = 'Violin Plot of Sulphates', box = True))\n",
        "fig.update_layout(title_x=0.5)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "R_ffVaO0sZqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#When performing Label Encoding below, you must encode train and test together as in\n",
        "df = pd.concat([train[col],test[col]],axis=0)\n",
        "# PERFORM FEATURE ENGINEERING HERE\n",
        "train[col] = df[:len(train)]\n",
        "test[col] = df[len(train):]"
      ],
      "metadata": {
        "id": "-GoBSeTf9BcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequency Encoding\n",
        "#Frequency encoding is a powerful technique that allows LGBM to see whether column values are rare or common. For example, if you want LGBM to \"see\" which credit cards are used infrequently, try\n",
        "\n",
        "temp = df['card1'].value_counts().to_dict()\n",
        "df['card1_counts'] = df['card1'].map(temp)"
      ],
      "metadata": {
        "id": "oeu9UxfC-YAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical encoding.\n",
        "from sklearn import preprocessing\n",
        "enc=preprocessing.LabelEncoder()\n",
        "#Label encoder from sklearn does not handle NaN values so NaN values should be imputed before implementing label encoder.\n",
        "df.loc[:,'col']=df.col.fillna('None')\n",
        "df.loc[:,'col']=enc.fit_tranform(df.col.values)"
      ],
      "metadata": {
        "id": "xsi111Pezt2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregations / Group Statistics\n",
        "#Providing LGBM with group statistics allows LGBM to determine if a value is common or rare for a particular group. You calculate group statistics by providing pandas with 3 variables. \n",
        "#You give it the group, variable of interest, and type of statistic. For example,\n",
        "\n",
        "temp = df.groupby('card1')['TransactionAmt'].agg(['mean'])   \n",
        "    .rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
        "df = pd.merge(df,temp,on='card1',how='left')"
      ],
      "metadata": {
        "id": "Bpkd_dVx_cBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw bar chart of missing values for train and test set\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n",
        "sns.barplot(x=null_values_train.index, y=null_values_train.values, ax=ax[0]).set_title('Null values percentage in train set')\n",
        "sns.barplot(x=null_values_train.index, y=null_values_test.values, ax=ax[1]).set_title('Null values percentage in test set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqSWYh8kELTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group columns based on column names\n",
        "\n",
        "cont_d = [col for col in train.columns.tolist() if col.startswith('D') and col not in cat_cols]\n",
        "cont_s = [col for col in train.columns.tolist() if col.startswith('S') and col not in cat_cols]\n",
        "cont_p = [col for col in train.columns.tolist() if col.startswith('P') and col not in cat_cols]\n",
        "cont_b = [col for col in train.columns.tolist() if col.startswith('B') and col not in cat_cols]\n",
        "cont_r = [col for col in train.columns.tolist() if col.startswith('R') and col not in cat_cols]"
      ],
      "metadata": {
        "id": "HtANBdm6xVmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace string with space\n",
        "x=re.sub(r'(<em>|</em>|<br/>)', '', all_rhymes)"
      ],
      "metadata": {
        "id": "1vV0ONC_PySk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create slice of prefetched dataset\n",
        "ds_subset = raw_train_ds.take(10) #returns first 10 batch, if the data has batched\n",
        "\n",
        "for data_batch in ds_subset:\n",
        "    #do whatever you want with each batch\n",
        "  \n",
        "#if you want to get examples, not batches:\n",
        "ds_subset = raw_train_ds.unbatch().take(320) #returns first 320 examples\n",
        "\n",
        "for input, label in ds_subset:\n",
        "   #do whatever you want with each sample (input,label)"
      ],
      "metadata": {
        "id": "EAPkW2scMyBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback that streams epoch results to a CSV file. In my opinion, it's the best. No need to worry about any variable. A CSV fill will be saved and updated at each training epoch.\n",
        "func_model.fit(x_train, y_train, \n",
        "               batch_size=256, \n",
        "               epochs=10, verbose = 2, \n",
        "               callbacks=[tf.keras.callbacks.CSVLogger('his.csv')])\n",
        "\n",
        "import pandas\n",
        "his = pandas.read_csv('his.csv') \n",
        "his.head()"
      ],
      "metadata": {
        "id": "Bquw5fxCReP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use of pivot tables to show aggregate data\n",
        "\n",
        "_pivot_1 = pd.pivot_table(index=['AgeBins'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')\n",
        "_pivot_2 = pd.pivot_table(index=['HomePlanet', 'AgeBins'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')\n",
        "_pivot_3 = pd.pivot_table(index=['AgeBins', 'HomePlanet'], columns=['Destination'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu', axis=0)\n",
        "_pivot_4 = pd.pivot_table(index=['AgeBins'], columns=['CabinDeck'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu', axis=0)\n",
        "\n",
        "multi_table([_pivot_1, _pivot_2, _pivot_3, _pivot_4])"
      ],
      "metadata": {
        "id": "OdSh0V-ZoAeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        ">>> y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        ">>> print(\"Number of mislabeled points out of a total %d points : %d\"\n",
        "...       % (X_test.shape[0], (y_test != y_pred).sum()))\n",
        "Number of mislabeled points out of a total 75 points : 4"
      ],
      "metadata": {
        "id": "jjjgKNmpk3x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterative imputation refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted.\n",
        "# Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "data=df.copy()\n",
        "\n",
        "#setting up the imputer\n",
        "\n",
        "imp = IterativeImputer(\n",
        "    estimator=xgboost.XGBRegressor(\n",
        "        n_estimators=5,\n",
        "        random_state=1,\n",
        "        tree_method='gpu_hist',\n",
        "    ),\n",
        "    missing_values=np.nan,\n",
        "    max_iter=5,\n",
        "    initial_strategy='mean',\n",
        "    imputation_order='ascending',\n",
        "    verbose=2,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "data[:] = imp.fit_transform(data)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "ygz40Miyhr_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach.\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "nan = np.nan\n",
        "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
        "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
        "imputed_x=imputer.fit_transform(X)\n",
        "print(X,imputed_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnRKCHGIiW5A",
        "outputId": "5f630bff-2529-4546-a72f-b5f745dcf0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]] [[1.  2.  4. ]\n",
            " [3.  4.  3. ]\n",
            " [5.5 6.  5. ]\n",
            " [8.  8.  7. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which variables have missing values\n",
        "\n",
        "columns_with_missing_values = housepricesdata.columns[housepricesdata.isnull().any()]\n",
        "housepricesdata[columns_with_missing_values].isnull().sum()"
      ],
      "metadata": {
        "id": "DfGHZCkOFQ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#when you add a new column to a DataFrame, you must\n",
        "#use dictionary syntax, like this\n",
        "# CORRECT\n",
        "df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0\n",
        "Not dot notation, like this:\n",
        "# WRONG!\n",
        "df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0"
      ],
      "metadata": {
        "id": "TixkENhCFQdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from github. Use raw format and copy url# Note normal url and raw url will be different.\n",
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/PacktPublishing/Ensemble-Machine-Learning-Cookbook/master/Chapter01/HousePrices.csv'\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "#df = pd.read_csv(url)\n",
        "hp.to_csv(df)\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZABviYuEpU",
        "outputId": "3ffefa29-073c-438b-8faf-96cb4d86bc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "Id                                                                    \n",
            "1           60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "2           20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "3           60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "4           70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "5           60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "   LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
            "Id                                  ...                                     \n",
            "1          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
            "2          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
            "3          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
            "4          Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
            "5          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
            "\n",
            "   MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "Id                                                             \n",
            "1        0      2    2008        WD         Normal     208500  \n",
            "2        0      5    2007        WD         Normal     181500  \n",
            "3        0      9    2008        WD         Normal     223500  \n",
            "4        0      2    2006        WD        Abnorml     140000  \n",
            "5        0     12    2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 80 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from github. Use raw format and copy url# Note normal url and raw url will be different.\n",
        "import pandas as pd\n",
        "pd.options.display.max_rows=None\n",
        "pd.options.display.max_columns=None\n",
        "url = 'https://raw.githubusercontent.com/PacktPublishing/Ensemble-Machine-Learning-Cookbook/master/Chapter01/HousePrices.csv'\n",
        "housepricesdata = pd.read_csv(url)\n",
        "#df = pd.read_csv(url)\n",
        "print(housepricesdata.head(5))"
      ],
      "metadata": {
        "id": "elBAffAVvSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find accuracy of regression model from sklearn\n",
        "\n",
        "from sklearn import metrics\n",
        "mae=metrics.mean_absolute_error(Y_test, Y_predicted)\n",
        "mse=metrics.mean_squared_error(Y_test,Y_predicted)\n",
        "r2_score=metrics.r2_score(Y_test, Y_predicted)"
      ],
      "metadata": {
        "id": "9yBkFAyYnu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train & test sets with stratification factor same as in labels\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, stratify=Y)"
      ],
      "metadata": {
        "id": "nwzwbiqu6jXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the categorial data by type\n",
        "df_messages.groupby('labels').describe()\n",
        "message\n",
        "count\tunique\ttop\tfreq\n",
        "labels\t\t\t\t\n",
        "ham\t4825\t4516\tSorry, I'll call later\t30\n",
        "spam\t747\t653\tPlease call our customer service representativ...\t4"
      ],
      "metadata": {
        "id": "BZ1qiGwPlymw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation \n",
        "square-root for moderate skew: sqrt(x) for positively skewed data, sqrt(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "log for greater skew: log10(x) for positively skewed data, log10(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "inverse for severe skew: 1/x for positively skewed data 1/(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "Linearity and heteroscedasticity: First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values If your data does the opposite – dependent variable values decrease more rapidly with increasing independent variable values – you can first consider a square transformation."
      ],
      "metadata": {
        "id": "njCfZAYQ_bSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Cross Validation \n",
        "Machine learning is an iterative process.You will face choices about predictive variables to use, what types of models to use,what arguments to supply those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives. You've already learned to use train_test_split to split the data, so you can measure model quality on the test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run.\n",
        "The Shortcoming of Train-Test Split \n",
        "Imagine you have a dataset with 5000 rows. The train_test_split function has an argument for test_size that you can use to decide how many rows go to the training set and how many go to the test set. The larger the test set, the more reliable your measures of model quality will be. At an extreme, you could imagine having only 1 row of data in the test set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck.\n",
        "You will typically keep about 20% as a test dataset. But even with 1000 rows in the test set, there's some random chance in determining model scores. A model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows. The larger the test set, the less randomness (aka \"noise\") there is in our measure of model quality.\n",
        "But we can only get a large test set by removing data from our training data, and smaller training datasets mean worse models. In fact, the ideal modeling decisions on a small dataset typically aren't the best modeling decisions on large datasets.\n",
        "The Cross-Validation Procedure \n",
        "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we could have 5 folds or experiments. We divide the data into 5 pieces, each being 20% of the full dataset.\n",
        "Cross Validation.png\n",
        "\n",
        "We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we got from using the simple train-test split.\n",
        "We then run a second experiment, where we hold out data from the second fold (using everything except the 2nd fold for training the model.) This gives us a second estimate of model quality. We repeat this process, using every fold once as the holdout. Putting this together, 100% of the data is used as a holdout at some point.\n",
        "Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously.\n",
        "Trade-offs Between Cross-Validation and Train-Test Split \n",
        "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
        "Given these tradeoffs, when should you use each approach? On small datasets, the extra computational burden of running cross-validation isn't a big deal. These are also the problems where model quality scores would be least reliable with train-test split. So, if your dataset is smaller, you should run cross-validation.\n",
        "For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
        "There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth.\n",
        "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient.\n",
        "Firstly, the choice of the number of folds should reflect your goals:\n",
        "\n",
        "If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). You can achieve this by using a higher number of folds, usually between 10 and 20.\n",
        "If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.\n",
        "Finally, if your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). Hence, a lower number of folds will suffice, usually between 3 and 5.\n"
      ],
      "metadata": {
        "id": "bSLgZgAyCPR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation using lasso regression, we can do same with grid search also\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "kwF_a-4b_WUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the log values for better comparision\n",
        "kaggle_rama_log_diff = \\\n",
        "    np.log(original_train_df[original_train_df.store == \"KaggleRama\"].num_sold.values).mean() - \\\n",
        "    np.log(original_train_df[original_train_df.store == \"KaggleMart\"].num_sold.values).mean()\n",
        "print(f\"KaggleRama always sells {np.exp(kaggle_rama_log_diff):.5f} more than KaggleMart.\")"
      ],
      "metadata": {
        "id": "h2bW70Ubkppm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unknown Dimension\n",
        "You are allowed to have one \"unknown\" dimension.\n",
        "\n",
        "Meaning that you do not have to specify an exact number for one of the dimensions in the reshape method.\n",
        "\n",
        "Pass -1 as the value, and NumPy will calculate this number for you."
      ],
      "metadata": {
        "id": "seZMEZlukwRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple unpivot columns\n",
        "# Transpose with multiple columns\n",
        "pd.melt(df, id_vars =['Name'], value_vars =['Course', 'Age'])\n",
        "\n",
        "# Names of ‘variable’ and ‘value’ columns can be customized\n",
        "pd.melt(df, id_vars =['Name'], value_vars =['Course'],\n",
        "              var_name ='ChangedVarname', value_name ='ChangedValname')"
      ],
      "metadata": {
        "id": "NkaM4FEKHjzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Add an indicator column while concatenating the two dataframes, so you can later seperate them again:\n",
        "\n",
        "df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])\n",
        "Then later you can split them again:\n",
        "\n",
        "test, train = df[df[\"ind\"].eq(\"test\")], df[df[\"ind\"].eq(\"train\")]"
      ],
      "metadata": {
        "id": "lRdQ0wmjfbV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A function to seed everything for getting reproducible results. '''\n",
        "    \n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = str(seed)\n",
        "    os.environ['TF_KERAS'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_all(42)"
      ],
      "metadata": {
        "id": "JvycK5vRjrG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial validation has been developed just for this purpose. It is a technique allowing you to easily estimate the degree of difference between your training and test data.\n",
        "The idea is simple: take your training data, remove the target, assemble your training data together with your test data, and create a new binary classification target where the positive label is assigned to the test data. At this point, run a machine learning classifier and evaluate for the ROC-AUC evaluation metric (we discussed this metric in the previous chapter on Detailing Competition Tasks and Metrics).\n",
        "\n",
        "If your ROC-AUC is around 0.5, it means that the training and test data are not easily distinguishable and are apparently from the same distribution. ROC-AUC values higher than 0.5 and nearing 1.0 signal that it is easy for the algorithm to figure out what is from the training set and what is from the test set: in such a case, don’t expect to be able to easily generalize to the test set because it clearly comes from a different distribution.\n",
        "https://www.kaggle.com/code/konradb/adversarial-validation-and-other-scary-terms/notebook"
      ],
      "metadata": {
        "id": "O4ZrjSAl0p3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all sub folders under directory\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
      ],
      "metadata": {
        "id": "2hGW4cAPxrWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most of the commands in the code you see on Kaggle Notebooks, you will find a parameter declaring a number, a seed, as the random state. This setting is important for the reproducibility of your results. Since many algorithms are not deterministic but are based on randomness, by setting a seed you influence the behavior of the random generator, making it predictable in its randomness: the same random seed corresponds to the same sequence of random numbers. In other words, it allows you to obtain the same results after every run of the same code.Again, reproducibility can be used to your advantage when dealing with public Notebooks. Most often, these Notebooks will have a fixed seed that could be 0, 1, or 42. The value 42 is quite popular because it is a reference to Douglas Adam’s The Hitchhiker’s Guide to the Galaxy, in which it is the “Answer to the Ultimate Question of Life, the Universe, and Everything,” calculated by an enormous supercomputer named Deep Thought over a period of 7.5 million years. Now, if everyone in a competition is using the same random seed, it could have a double effect:"
      ],
      "metadata": {
        "id": "uzSb-YUiHwbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "In an EDA for a Kaggle competition, you will be looking for:\n",
        "\n",
        "Missing values and, most importantly, missing value patterns correlated with the target.\n",
        "Skewed numeric variables and their possible transformations.\n",
        "Rare categories in categorical variables that can be grouped together.\n",
        "Potential outliers, both univariate and multivariate.\n",
        "Highly correlated (or even duplicated) features. For categorical variables, focus on categories that overlap.\n",
        "The most predictive features for the problem."
      ],
      "metadata": {
        "id": "IYydd6qlJYGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, as a piece of advice based on our experience, don’t expect a neural network to be the best model in a tabular competition; this seldom happens. Instead, blend solutions from classical tabular data models, such as gradient boosting models and neural networks, because they tend to pick up different signals from the data that you can integrate together in an ensemble."
      ],
      "metadata": {
        "id": "oVEP8iQdaE4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear models\n",
        "The linear models that need to be tuned are usually linear regressions or logistic regressions with regularization:\n",
        "\n",
        "C: The range you should search is np.logspace(-4, 4, 10); smaller values specify stronger regularization.\n",
        "alpha: You should search the range np.logspace(-2, 2, 10); smaller values specify stronger regularization, larger values specify stronger regularization. Also take note that higher values take more time to process when using lasso.\n",
        "l1_ratio: You should pick from the list [.1, .5, .7, .9, .95, .99, 1]; it applies only to elastic net."
      ],
      "metadata": {
        "id": "nfYzFA40fu5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KIkmo-cAfxQX"
      }
    }
  ]
}