{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MuVcvb7zuAmw"
      ],
      "authorship_tag": "ABX9TyMdKV/eAHGRhwZMDRWpKZlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f3446088b4246febba3d93c40adafdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94de8b9042d14ce591d8a22d2ccfdf5b",
              "IPY_MODEL_65a69944eb6d4f5783180c01273883df",
              "IPY_MODEL_9bc8ea3812504ed2987563905c7379b8"
            ],
            "layout": "IPY_MODEL_93357a2e808b49618a905d2c32318b82"
          }
        },
        "94de8b9042d14ce591d8a22d2ccfdf5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b550ef78c141db873fa3dfeac8d728",
            "placeholder": "​",
            "style": "IPY_MODEL_9f72112b52734aba84604c04df39f0a0",
            "value": " 72%"
          }
        },
        "65a69944eb6d4f5783180c01273883df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e33e959b7ddb4b0d9e648768d6fa8e6e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb3aef622f0043eba9aae3a33060e655",
            "value": 72
          }
        },
        "9bc8ea3812504ed2987563905c7379b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a1364b405142e693514dba5699818f",
            "placeholder": "​",
            "style": "IPY_MODEL_551225791e994ee4a847fc868b085901",
            "value": " 72/100 [43:43&lt;32:03, 68.71s/it]"
          }
        },
        "93357a2e808b49618a905d2c32318b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b550ef78c141db873fa3dfeac8d728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f72112b52734aba84604c04df39f0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e33e959b7ddb4b0d9e648768d6fa8e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb3aef622f0043eba9aae3a33060e655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37a1364b405142e693514dba5699818f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551225791e994ee4a847fc868b085901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag314/EDA/blob/main/Useful_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas basics\n",
        "Delete rows with NAN values\n",
        ">1.axis: Default – 0\n",
        "0, or ‘index’ : Drop rows which contain NaN values.\n",
        "1, or ‘columns’ : Drop columns which contain NaN value.\n",
        ">2.how: Default – ‘any’\n",
        "‘any’ : Drop rows / columns which contain any NaN values.\n",
        "‘all’ : Drop rows / columns which contain all NaN values.\n",
        ">3.thresh (int): Optional\n",
        "Delete rows/columns which contains less than minimun thresh number of non-NaN values.\n",
        ">4.inplace (bool): Default- False\n",
        "If True, modifies the calling dataframe object\n",
        "\n",
        "\n",
        "Returns\n",
        "\n",
        "If inplace==True, the return None, else returns a new dataframe by deleting the rows/columns based on NaN values\n",
        "you must add inplace = True argument, if you want the dataframe to be actually updated."
      ],
      "metadata": {
        "id": "QP5HaKbEI5BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rename column\n",
        "cci.rename(columns={'Value':'cci'},inplace=True)"
      ],
      "metadata": {
        "id": "vv3xjob5CTw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforming between -1 to 1 instead of 0 to 1\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))"
      ],
      "metadata": {
        "id": "o-_ZMGVneDLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVJAF2xZIvHK"
      },
      "outputs": [],
      "source": [
        "#Pandas delete rows with NAN values\n",
        "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows which contain all NaN values\n",
        "df = df.dropna(axis=0, how='all')\n",
        "df.dropna(axis=1)# Drop columns that contain any NaN value.\n",
        "df.dropna(thresh=3,axis=1)# Drop any column that have less than 3 NaN values\n",
        "df.dropna(axis=column,how='all')# Dron columns that contain ALL NaN values."
      ],
      "metadata": {
        "id": "eEu26wPHK4fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#If you want to select rows with at least one NaN value, then you could use isna + any on axis=1:\n",
        "\n",
        "df[df.isna().any(axis=1)]\n",
        "#If you want to select rows with a certain number of NaN values, then you could use isna + sum on axis=1 + gt. For example, the following will fetch rows with at least 2 NaN values:\n",
        "\n",
        "df[df.isna().sum(axis=1)>1]\n",
        "\n",
        "# Select not NaN values\n",
        "df[df.notna().all(axis=1)]"
      ],
      "metadata": {
        "id": "lpwrXpCTM400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USe queary  instead of subsetting.\n",
        "min_year=1980\n",
        "min_time=10\n",
        "df=df.query('Year<@min_year and Time >@min_time')"
      ],
      "metadata": {
        "id": "v6X82NG-szIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert large datasets to small datasets\n",
        "large_df=pd.read_csv('large_dataset.csv')\n",
        "large_df=pd.to_parquet('output.parquet')\n",
        "large_df=pd.to_pickle('output.pickle')\n",
        "large_df=pd.to_feather('output.feather')"
      ],
      "metadata": {
        "id": "R7Yd2030tMWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate values\n",
        "df.duplicated().sum()\n",
        "# Keep first one of duplicate values\n",
        "df_eda[df_eda.duplicated(keep='first')]"
      ],
      "metadata": {
        "id": "WMhEl2-hxt9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print all columns names\n",
        "print(df.columns)\n",
        "#Print all columns types\n",
        "print(df.dtypes)\n",
        "# Select specific data types \n",
        "only_ints = df.select_dtypes(include=['int'])\n",
        "print(only_ints.columns)\n",
        "#rename dataset columns\n",
        "data.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n"
      ],
      "metadata": {
        "id": "1HLUQyzP768W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For a dataframe df, one can use any of the following: Number of rows in dataframe\n",
        "df.shape\n",
        "len(df.index)\n",
        "len(df.comlumns)# To find out number of columns\n",
        "df.shape[0]\n",
        "df[df.columns[0]].count() (== number of non-NaN values in first column)\n",
        "count_row = df.shape[0]  # Gives number of rows\n",
        "count_col = df.shape[1]  # Gives number of columns\n",
        "r, c = df.shape"
      ],
      "metadata": {
        "id": "s8OBL57nReev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas one hot encodding\n",
        "one_hot_encoded_data = pd.get_dummies(data, columns = ['Remarks', 'Gender'])"
      ],
      "metadata": {
        "id": "1cZGVbS6MTVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete rows with a condition.\n",
        "#Delete all rows if a row contains 3 missing values\n",
        "df.drop(df[df.isna().sum(axis=1)==3].index,inplace=True)\n",
        "\n",
        "#To remove all rows where column 'score' is < 50 and > 20\n",
        "#The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses.\n",
        "df = df.drop(df[(df.score < 50) & (df.score > 20)].index)\n",
        "\n",
        "# Drop record with specified values. Drop record where area >4000 and price , 210000\n",
        "df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<210000)].index)"
      ],
      "metadata": {
        "id": "r455qNCgaQT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NN output probability to boolean values.\n",
        "test2.loc[test2['Trans'] < 0.5, 'Transported'] = 'False' \n",
        "test2.loc[test2['Trans'] >= 0.5, 'Transported'] = 'True' \n",
        "test2.drop('Trans',axis=1,inplace=True)\n",
        "\n",
        "#Another way is to use replace function\n",
        "survivors_df['Survived'].replace([0], 'Not Survived', inplace = True)\n",
        "survivors_df['Survived'].replace([1], 'Survived', inplace = True)"
      ],
      "metadata": {
        "id": "B925e_ZhNz63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To find out unique values within columns\n",
        "x.iloc[:,3].unique() # All unique values in column 3\n",
        "temp.Pclass.unique()"
      ],
      "metadata": {
        "id": "_DxN-opXrWdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert numeric values to float\n",
        "numeric_list = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_list] = df[numeric_list].astype(np.float32)\n",
        "# Convert object type to numeric\n",
        "df[\"Total_Charges\"]=pd.to_numeric(df[\"Total_Charges\"])"
      ],
      "metadata": {
        "id": "SmxA_Q4j7jqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set floating point option to display 4 decimals.\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
      ],
      "metadata": {
        "id": "bnGqncNjtTtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many data is type of object\n",
        "\n",
        "df_data.select_dtypes(['object']).head()\n",
        "\n",
        "#Get all string columns \n",
        "string_cols = [f for f in train.columns if train[f].dtype == object]"
      ],
      "metadata": {
        "id": "dXICP9Ys3UQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the frequency of dataframe columns\n",
        "\n",
        "df1 = df['Courses'].value_counts()\n",
        "df1 = df.Courses.value_counts\n",
        "df1 = df.groupby('Courses').count()\n",
        "df1 = df.index.value_counts()"
      ],
      "metadata": {
        "id": "LF1rrZC4OCZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform all object and boolean column to integer\n",
        "for colname in x_train.select_dtypes(['object','bool']).columns:\n",
        "  x_train[colname]=LabelEncoder().fit_transform(x_train[colname])\n"
      ],
      "metadata": {
        "id": "Ar2vqdqa0_yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop whole column\n",
        "train=train.drop(['Age'].index,inplace=True)"
      ],
      "metadata": {
        "id": "4J-e26q5q5al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LEts check correlation matric to find out which features are important in prediction survival\n",
        "\n",
        "corrMatrix = train.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kFFbttYxvg0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the correlation between numerical variable with sale price\n",
        "corr=housepricesdata.corr()[\"SalePrice\"]\n",
        "\n",
        "# Sort the correlation values\n",
        "# Use [::-1] to sort it in descending manner\n",
        "# Use [::+1] to sort it in ascending manner\n",
        "corr[np.argsort(corr)[::-1]]"
      ],
      "metadata": {
        "id": "jPtwlwScriEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SalePrice         1.000000\n",
        "GrLivArea         0.708624\n",
        "GarageCars        0.640409\n",
        "GarageArea        0.623431\n",
        "TotalBsmtSF       0.613581\n",
        "1stFlrSF          0.605852\n",
        "FullBath          0.560664\n",
        "TotRmsAbvGrd      0.533723\n",
        "YearBuilt         0.522897\n",
        "YearRemodAdd      0.507101\n",
        "MasVnrArea        0.472614\n",
        "Fireplaces        0.466929\n",
        "BsmtFinSF1        0.386420\n",
        "LotFrontage       0.334771\n",
        "WoodDeckSF        0.324413\n",
        "2ndFlrSF          0.319334\n",
        "OpenPorchSF       0.315856\n",
        "HalfBath          0.284108\n",
        "LotArea           0.263843\n",
        "GarageYrBlt       0.261366\n",
        "BsmtFullBath      0.227122\n",
        "LotShape_IR1      0.223284\n",
        "BsmtUnfSF         0.214479\n",
        "BedroomAbvGr      0.168213\n",
        "LotShape_IR2      0.126096\n",
        "ScreenPorch       0.111447\n",
        "PoolArea          0.092404\n",
        "MoSold            0.046432\n",
        "3SsnPorch         0.044584\n",
        "LotShape_IR3      0.036720\n",
        "BsmtFinSF2       -0.011378\n",
        "BsmtHalfBath     -0.016844\n",
        "MiscVal          -0.021190\n",
        "LowQualFinSF     -0.025606\n",
        "YrSold           -0.028923\n",
        "EnclosedPorch    -0.128578\n",
        "KitchenAbvGr     -0.135907\n",
        "LotShape_Reg     -0.267672\n",
        "remodelled_age   -0.507101\n",
        "building_age     -0.522897\n",
        "Name: SalePrice, dtype: float64"
      ],
      "metadata": {
        "id": "yixQx-AVrrSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We check the correlation of each of our feature variables with the target variable\n",
        "df_creditcarddata.drop(['default.payment.next.month'], \\\n",
        "     axis = 1).corrwith(df_creditcarddata['default.payment.next.month']).\\\n",
        "     plot.bar(figsize=(20,10), \\\n",
        "     title = 'Correlation with Response variable', \\\n",
        "     fontsize = 15, rot = 45, grid = True)\n"
      ],
      "metadata": {
        "id": "hdQg5BRH7Ium"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross tab frequency table\n",
        "train.groupby(['Fare', 'Survived'])['Fare'].count().unstack()"
      ],
      "metadata": {
        "id": "kbkpZjDhyC45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of min max scaler in data which is not normal and has a range of values such as pixel values\n",
        "# Use of standard scaler when data is normally distributed.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler()\n",
        "train['Fare']=scaler.fit_transform(train[['Fare']])"
      ],
      "metadata": {
        "id": "W7p2mPlb4Tto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to find median\n",
        "median = df['Age'].describe()[5]\n",
        "\n",
        "#If you get your output in scientific notation, you can change to view it in standard form instead by\n",
        "# executing the following command: pd.options.display.float_format = ‘{:.2f}’.format"
      ],
      "metadata": {
        "id": "FJBM-lcRDarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of describe use summary info\n",
        "flifrom summarytools import dfSummary\n",
        "dfSummary(iris_df)"
      ],
      "metadata": {
        "id": "ZMmMLS4uugeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values with mode for fare in test data\n",
        "\n",
        "mode=train['Embarked'].value_counts().index[0]\n",
        "train['Embarked']=train['Embarked'].fillna(mode)\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "J0IPuN0H6nrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw barplot with cross tab freq values.\n",
        "ax=sns.countplot(data=temp,x='Pclass',hue='Survived')\n",
        "ax.bar_label(ax.containers[0])\n",
        "ax.bar_label(ax.containers[1])\n",
        "plt.legend(title='Survived or not',loc='upper left',labels=['No','Yes'])"
      ],
      "metadata": {
        "id": "Smai-73RpsDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to categories categories\n",
        "  data['Age']=pd.cut(data['Age'],bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])"
      ],
      "metadata": {
        "id": "TorY3W_9yqAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics from history object\n",
        "\n",
        "train_loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "train_accuracy=history.history['accuracy']\n",
        "val_accuracy=history.history['val_accuracy']\n",
        "epochs=range(len(train_accuracy))\n",
        "#plot loss\n",
        "plt.plot(epochs,train_loss,color='b',label='Train loss')\n",
        "plt.plot(epochs,val_loss,color='r',label='Validation loss')\n",
        "#plot accuracy\n",
        "plt.plot(epochs,train_accuracy,color='b',label='Train accuracy')\n",
        "plt.plot(epochs,val_accuracy,color='r',label='Validation accuracy')"
      ],
      "metadata": {
        "id": "gRlNb_aN-3PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGB model \n",
        "xgb_model=xgb.XGBClassifier(objective='binary:logistic',random_state=42)\n",
        "\n",
        "xgb_model.fit(tr_x,tr_y)\n",
        "y_pred=xgb_model.predict(cv_x)\n",
        "print(confusion_matrix(cv_y,y_pred))"
      ],
      "metadata": {
        "id": "L0qS_ZY6U1Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the feature importances \n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4R04_ZjJqQTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list comprehension Square all values in list x\n",
        "num**2 for num in x\n",
        "\n",
        "values=[expression for value in collection if condition]\n",
        "# Conditionals on iterable\n",
        "num1 = [5,10,15]\n",
        "num2 = [i**2 if i == 10 else i-5 if i < 7 else i+5 for i in num1]\n",
        "print(num2)\n",
        "# List comprehension is also used to create dictionaries\n",
        "names=['Bruce','Clark','Peter','Logan','Wade']\n",
        "heros=['Batman','Superman','Spiderman','Wolverine','Deadpool']\n",
        "my_dict={name:hero for name,hero in zip(names,heros)}\n",
        "# lambda function to square all values \n",
        "list(map(lambda x: x**2,range(1,6)))\n",
        "\n",
        "#argmax() and argmin() are used to find index location of max and min values in numpy array.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-hjNTPlnLDC",
        "outputId": "6ef411cb-bd40-4510-bade-42f6e6cc6b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "arr=np.arange(0,5)\n",
        "arr\n",
        "arr/arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgNIAKCx0BzG",
        "outputId": "76a22084-1c38-472b-d197-c6ea99e0e5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  1.,  1.,  1.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting index with col_name\n",
        "df.set_index('col_name',inplace=True)\n",
        "\n",
        "# Reseting index of the dataframe to numerical index(which is default index)\n",
        "\n",
        "df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "bngYLJTwhBoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict={'A':[1,2 ,np.nan,4,np.nan],\n",
        "           'B':[np.nan, np.nan,np.nan,np.nan,np.nan],\n",
        "           'C':[11,12,13,14,15],\n",
        "           'D':[16,np.nan,18,19,20]}\n",
        "df=pd.DataFrame(data_dict)\n",
        "df.fillna(method='ffill')# Use the last valid observation to fill the gap\n",
        "df.fillna(method='bfill')#Use next valid observation to fill the gap\n"
      ],
      "metadata": {
        "id": "3Dtd-xsMdt1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge datasets joins\n",
        "pd.merge(df1,df2,how='inner',on=['key'])\n",
        "pd.merge(df1,df2,how='outer',on=['key'])\n",
        "pd.merge(df1,df2,how='left',on=['key'])\n",
        "pd.merge(df1,df2,how='right',on=['key'])\n",
        "\n",
        "#Stack datasets\n",
        "pd.concat([df1],[df2])"
      ],
      "metadata": {
        "id": "fEhroj9hnTxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two features with highest chi-squared statistics are selected\n",
        "#For selecting categorical features .Change k for number of features.\n",
        "chi2_features = SelectKBest(chi2, k = 2)\n",
        "X_kbest_features = chi2_features.fit_transform(X, y)"
      ],
      "metadata": {
        "id": "Ro7o8yEKIMTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print importnant features in the model\n",
        "\n",
        "model=ExtraTreesClassifier()\n",
        "model.fit(x,y)\n",
        "print(model.feature_importances_)\n",
        "feature_importances=pd.Series(model.feature_importanes_,index=x.columns)\n",
        "feature_importances.nlargest(10).plot(kind='barh')# Draw horizontal bar graph of 10 most important features.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8SshKKVcYkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The correlation matrix shows only linear dependence. If we plot a rolling mean of the target probability for every feature, we'll see nonlinear dependences as well:\n",
        "# If we get non linear dependencies then \n",
        "ax.scatter(temp[f], temp.state.rolling(15000, center=True).mean(), s=2)# Plot rolling mean of 15000 values against feature ."
      ],
      "metadata": {
        "id": "ALF0SR2SwHSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use set_index and reset_index to find out how many family memebers from family name\n",
        "test_df['FamilyId'] = test_df['PassengerId'].str.split(\"_\", n=2, expand=True)[0]\n",
        "test_df['Family Name'] = test_df['Name'].str.split(' ', n=2, expand=True)[1]\n",
        "test_df = test_df.set_index(['FamilyId','Family Name'])# Setting the index with FamilyId and Family Name witll set index as an example below\n",
        "test_df['Family Size'] = 1 # Default family size\n",
        "​\n",
        "for i in range(test_df.shape[0]):\n",
        "    fam_size = test_df.loc[test_df.index[i],:].shape[0]# this gives list of (number of family members, total columns in test datasets)--> shape[0] will give total familly members\n",
        "    test_df.loc[test_df.index[i],'Family Size'] = fam_size\n",
        "​\n",
        "test_df=test_df.reset_index()# Reset index to default"
      ],
      "metadata": {
        "id": "Vu94XFwAa1Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering dataset\n",
        "#Check if countries are in column \n",
        "\n",
        "countries=[\"United States\",\"India\",\"United Kingdon\",\"Germany\",\"Canada\"]\n",
        "filt=df['Country'].isin(countries)\n",
        "df.loc[filt,'Country']"
      ],
      "metadata": {
        "id": "fnVAG42d1e6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find sum across columns \n",
        "df[['a','b','d']].agg('sum', axis=1)\n",
        "#The advantage of agg is that you can use multiple aggregation functions:\n",
        "\n",
        "df[['a','b','d']].agg(['sum', 'prod', 'min', 'max'], axis=1)"
      ],
      "metadata": {
        "id": "fnzCqO_yD3RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering based on if else condition. Create/update column based on existing column conditions.\n",
        "df['Interest']=[i*0.02 if i<20 else i*0.04 for i in df.debts]"
      ],
      "metadata": {
        "id": "RhsrOuyQRW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To achieve a compact visualization of the distribution that retains histogram-like characteristics, Hintze and Nelson (1998) developed the violin plot (c).\n",
        "#Violin plot is created by generating a density or distribution of the data and its mirror image. In the above figure we can see the violin plot, where we can\n",
        " #now see the many distinct peaks in citric acid distribution. The lower quartile, median, and upper quartile can be added to a violin plot to also consider \n",
        "# this information in the overall assessment of the distribution.\n",
        " fig = go.Figure(px.violin(df, y = 'sulphates', title = 'Violin Plot of Sulphates', box = True))\n",
        "fig.update_layout(title_x=0.5)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "R_ffVaO0sZqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#When performing Label Encoding below, you must encode train and test together as in\n",
        "df = pd.concat([train[col],test[col]],axis=0)\n",
        "# PERFORM FEATURE ENGINEERING HERE\n",
        "train[col] = df[:len(train)]\n",
        "test[col] = df[len(train):]"
      ],
      "metadata": {
        "id": "-GoBSeTf9BcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequency Encoding\n",
        "#Frequency encoding is a powerful technique that allows LGBM to see whether column values are rare or common. For example, if you want LGBM to \"see\" which credit cards are used infrequently, try\n",
        "\n",
        "temp = df['card1'].value_counts().to_dict()\n",
        "df['card1_counts'] = df['card1'].map(temp)"
      ],
      "metadata": {
        "id": "oeu9UxfC-YAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical encoding.\n",
        "from sklearn import preprocessing\n",
        "enc=preprocessing.LabelEncoder()\n",
        "#Label encoder from sklearn does not handle NaN values so NaN values should be imputed before implementing label encoder.\n",
        "df.loc[:,'col']=df.col.fillna('None')\n",
        "df.loc[:,'col']=enc.fit_tranform(df.col.values)"
      ],
      "metadata": {
        "id": "xsi111Pezt2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregations / Group Statistics\n",
        "#Providing LGBM with group statistics allows LGBM to determine if a value is common or rare for a particular group. You calculate group statistics by providing pandas with 3 variables. \n",
        "#You give it the group, variable of interest, and type of statistic. For example,\n",
        "\n",
        "temp = df.groupby('card1')['TransactionAmt'].agg(['mean'])   \n",
        "    .rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
        "df = pd.merge(df,temp,on='card1',how='left')"
      ],
      "metadata": {
        "id": "Bpkd_dVx_cBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw bar chart of missing values for train and test set\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n",
        "sns.barplot(x=null_values_train.index, y=null_values_train.values, ax=ax[0]).set_title('Null values percentage in train set')\n",
        "sns.barplot(x=null_values_train.index, y=null_values_test.values, ax=ax[1]).set_title('Null values percentage in test set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqSWYh8kELTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group columns based on column names\n",
        "\n",
        "cont_d = [col for col in train.columns.tolist() if col.startswith('D') and col not in cat_cols]\n",
        "cont_s = [col for col in train.columns.tolist() if col.startswith('S') and col not in cat_cols]\n",
        "cont_p = [col for col in train.columns.tolist() if col.startswith('P') and col not in cat_cols]\n",
        "cont_b = [col for col in train.columns.tolist() if col.startswith('B') and col not in cat_cols]\n",
        "cont_r = [col for col in train.columns.tolist() if col.startswith('R') and col not in cat_cols]"
      ],
      "metadata": {
        "id": "HtANBdm6xVmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace string with space\n",
        "x=re.sub(r'(<em>|</em>|<br/>)', '', all_rhymes)"
      ],
      "metadata": {
        "id": "1vV0ONC_PySk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create slice of prefetched dataset\n",
        "ds_subset = raw_train_ds.take(10) #returns first 10 batch, if the data has batched\n",
        "\n",
        "for data_batch in ds_subset:\n",
        "    #do whatever you want with each batch\n",
        "  \n",
        "#if you want to get examples, not batches:\n",
        "ds_subset = raw_train_ds.unbatch().take(320) #returns first 320 examples\n",
        "\n",
        "for input, label in ds_subset:\n",
        "   #do whatever you want with each sample (input,label)"
      ],
      "metadata": {
        "id": "EAPkW2scMyBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback that streams epoch results to a CSV file. it's the best. No need to worry about any variable. A CSV fill will be saved and updated at each training epoch.\n",
        "func_model.fit(x_train, y_train, \n",
        "               batch_size=256, \n",
        "               epochs=10, verbose = 2, \n",
        "               callbacks=[tf.keras.callbacks.CSVLogger('his.csv')])\n",
        "\n",
        "import pandas\n",
        "his = pandas.read_csv('his.csv') \n",
        "his.head()"
      ],
      "metadata": {
        "id": "Bquw5fxCReP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use of pivot tables to show aggregate data\n",
        "\n",
        "_pivot_1 = pd.pivot_table(index=['AgeBins'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')\n",
        "_pivot_2 = pd.pivot_table(index=['HomePlanet', 'AgeBins'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')\n",
        "_pivot_3 = pd.pivot_table(index=['AgeBins', 'HomePlanet'], columns=['Destination'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu', axis=0)\n",
        "_pivot_4 = pd.pivot_table(index=['AgeBins'], columns=['CabinDeck'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu', axis=0)\n",
        "\n",
        "multi_table([_pivot_1, _pivot_2, _pivot_3, _pivot_4])"
      ],
      "metadata": {
        "id": "OdSh0V-ZoAeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        ">>> y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        ">>> print(\"Number of mislabeled points out of a total %d points : %d\"\n",
        "...       % (X_test.shape[0], (y_test != y_pred).sum()))\n",
        "Number of mislabeled points out of a total 75 points : 4"
      ],
      "metadata": {
        "id": "jjjgKNmpk3x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterative imputation refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted.\n",
        "# Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "data=df.copy()\n",
        "\n",
        "#setting up the imputer\n",
        "\n",
        "imp = IterativeImputer(\n",
        "    estimator=xgboost.XGBRegressor(\n",
        "        n_estimators=5,\n",
        "        random_state=1,\n",
        "        tree_method='gpu_hist',\n",
        "    ),\n",
        "    missing_values=np.nan,\n",
        "    max_iter=5,\n",
        "    initial_strategy='mean',\n",
        "    imputation_order='ascending',\n",
        "    verbose=2,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "data[:] = imp.fit_transform(data)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "ygz40Miyhr_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach.\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "nan = np.nan\n",
        "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
        "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
        "imputed_x=imputer.fit_transform(X)\n",
        "print(X,imputed_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnRKCHGIiW5A",
        "outputId": "5f630bff-2529-4546-a72f-b5f745dcf0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]] [[1.  2.  4. ]\n",
            " [3.  4.  3. ]\n",
            " [5.5 6.  5. ]\n",
            " [8.  8.  7. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which variables have missing values\n",
        "\n",
        "columns_with_missing_values = housepricesdata.columns[housepricesdata.isnull().any()]\n",
        "housepricesdata[columns_with_missing_values].isnull().sum()"
      ],
      "metadata": {
        "id": "DfGHZCkOFQ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#when you add a new column to a DataFrame, you must\n",
        "#use dictionary syntax, like this\n",
        "# CORRECT\n",
        "df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0\n",
        "Not dot notation, like this:\n",
        "# WRONG!\n",
        "df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0"
      ],
      "metadata": {
        "id": "TixkENhCFQdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from github. Use raw format and copy url# Note normal url and raw url will be different.\n",
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/PacktPublishing/Ensemble-Machine-Learning-Cookbook/master/Chapter01/HousePrices.csv'\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "#df = pd.read_csv(url)\n",
        "hp.to_csv(df)\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZABviYuEpU",
        "outputId": "3ffefa29-073c-438b-8faf-96cb4d86bc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "Id                                                                    \n",
            "1           60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "2           20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "3           60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "4           70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "5           60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "   LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
            "Id                                  ...                                     \n",
            "1          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
            "2          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
            "3          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
            "4          Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
            "5          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
            "\n",
            "   MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "Id                                                             \n",
            "1        0      2    2008        WD         Normal     208500  \n",
            "2        0      5    2007        WD         Normal     181500  \n",
            "3        0      9    2008        WD         Normal     223500  \n",
            "4        0      2    2006        WD        Abnorml     140000  \n",
            "5        0     12    2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 80 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from github. Use raw format and copy url# Note normal url and raw url will be different.\n",
        "import pandas as pd\n",
        "pd.options.display.max_rows=None\n",
        "pd.options.display.max_columns=None\n",
        "url = 'https://raw.githubusercontent.com/PacktPublishing/Ensemble-Machine-Learning-Cookbook/master/Chapter01/HousePrices.csv'\n",
        "housepricesdata = pd.read_csv(url)\n",
        "#df = pd.read_csv(url)\n",
        "print(housepricesdata.head(5))"
      ],
      "metadata": {
        "id": "elBAffAVvSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find accuracy of regression model from sklearn\n",
        "\n",
        "from sklearn import metrics\n",
        "mae=metrics.mean_absolute_error(Y_test, Y_predicted)\n",
        "mse=metrics.mean_squared_error(Y_test,Y_predicted)\n",
        "r2_score=metrics.r2_score(Y_test, Y_predicted)"
      ],
      "metadata": {
        "id": "9yBkFAyYnu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train & test sets with stratification factor same as in labels\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, stratify=Y)"
      ],
      "metadata": {
        "id": "nwzwbiqu6jXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the categorial data by type\n",
        "df_messages.groupby('labels').describe()\n",
        "message\n",
        "count\tunique\ttop\tfreq\n",
        "labels\t\t\t\t\n",
        "ham\t4825\t4516\tSorry, I'll call later\t30\n",
        "spam\t747\t653\tPlease call our customer service representativ...\t4"
      ],
      "metadata": {
        "id": "BZ1qiGwPlymw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation \n",
        "square-root for moderate skew: sqrt(x) for positively skewed data, sqrt(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "log for greater skew: log10(x) for positively skewed data, log10(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "inverse for severe skew: 1/x for positively skewed data 1/(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "Linearity and heteroscedasticity: First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values If your data does the opposite – dependent variable values decrease more rapidly with increasing independent variable values – you can first consider a square transformation."
      ],
      "metadata": {
        "id": "njCfZAYQ_bSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Cross Validation \n",
        "Machine learning is an iterative process.You will face choices about predictive variables to use, what types of models to use,what arguments to supply those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives. You've already learned to use train_test_split to split the data, so you can measure model quality on the test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run.\n",
        "The Shortcoming of Train-Test Split \n",
        "Imagine you have a dataset with 5000 rows. The train_test_split function has an argument for test_size that you can use to decide how many rows go to the training set and how many go to the test set. The larger the test set, the more reliable your measures of model quality will be. At an extreme, you could imagine having only 1 row of data in the test set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck.\n",
        "You will typically keep about 20% as a test dataset. But even with 1000 rows in the test set, there's some random chance in determining model scores. A model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows. The larger the test set, the less randomness (aka \"noise\") there is in our measure of model quality.\n",
        "But we can only get a large test set by removing data from our training data, and smaller training datasets mean worse models. In fact, the ideal modeling decisions on a small dataset typically aren't the best modeling decisions on large datasets.\n",
        "The Cross-Validation Procedure \n",
        "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we could have 5 folds or experiments. We divide the data into 5 pieces, each being 20% of the full dataset.\n",
        "Cross Validation.png\n",
        "\n",
        "We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we got from using the simple train-test split.\n",
        "We then run a second experiment, where we hold out data from the second fold (using everything except the 2nd fold for training the model.) This gives us a second estimate of model quality. We repeat this process, using every fold once as the holdout. Putting this together, 100% of the data is used as a holdout at some point.\n",
        "Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously.\n",
        "Trade-offs Between Cross-Validation and Train-Test Split \n",
        "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
        "Given these tradeoffs, when should you use each approach? On small datasets, the extra computational burden of running cross-validation isn't a big deal. These are also the problems where model quality scores would be least reliable with train-test split. So, if your dataset is smaller, you should run cross-validation.\n",
        "For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
        "There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth.\n",
        "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient.\n",
        "Firstly, the choice of the number of folds should reflect your goals:\n",
        "\n",
        "If your purpose is performance estimation, you need models with low bias estimates (which means no systematic distortion of estimates). You can achieve this by using a higher number of folds, usually between 10 and 20.\n",
        "If your aim is parameter tuning, you need a mix of bias and variance, so it is advisable to use a medium number of folds, usually between 5 and 7.\n",
        "Finally, if your purpose is just to apply variable selection and simplify your dataset, you need models with low variance estimates (or you will have disagreement). Hence, a lower number of folds will suffice, usually between 3 and 5.\n"
      ],
      "metadata": {
        "id": "bSLgZgAyCPR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation using lasso regression, we can do same with grid search also\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "kwF_a-4b_WUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the log values for better comparision\n",
        "kaggle_rama_log_diff = \\\n",
        "    np.log(original_train_df[original_train_df.store == \"KaggleRama\"].num_sold.values).mean() - \\\n",
        "    np.log(original_train_df[original_train_df.store == \"KaggleMart\"].num_sold.values).mean()\n",
        "print(f\"KaggleRama always sells {np.exp(kaggle_rama_log_diff):.5f} more than KaggleMart.\")"
      ],
      "metadata": {
        "id": "h2bW70Ubkppm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unknown Dimension\n",
        "You are allowed to have one \"unknown\" dimension.\n",
        "\n",
        "Meaning that you do not have to specify an exact number for one of the dimensions in the reshape method.\n",
        "\n",
        "Pass -1 as the value, and NumPy will calculate this number for you."
      ],
      "metadata": {
        "id": "seZMEZlukwRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple unpivot columns\n",
        "# Transpose with multiple columns\n",
        "pd.melt(df, id_vars =['Name'], value_vars =['Course', 'Age'])\n",
        "\n",
        "# Names of ‘variable’ and ‘value’ columns can be customized\n",
        "pd.melt(df, id_vars =['Name'], value_vars =['Course'],\n",
        "              var_name ='ChangedVarname', value_name ='ChangedValname')"
      ],
      "metadata": {
        "id": "NkaM4FEKHjzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Add an indicator column while concatenating the two dataframes, so you can later seperate them again:\n",
        "\n",
        "df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])\n",
        "Then later you can split them again:\n",
        "\n",
        "test, train = df[df[\"ind\"].eq(\"test\")], df[df[\"ind\"].eq(\"train\")]"
      ],
      "metadata": {
        "id": "lRdQ0wmjfbV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A function to seed everything for getting reproducible results. '''\n",
        "    \n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = str(seed)\n",
        "    os.environ['TF_KERAS'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_all(42)"
      ],
      "metadata": {
        "id": "JvycK5vRjrG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial validation has been developed just for this purpose. It is a technique allowing you to easily estimate the degree of difference between your training and test data.\n",
        "The idea is simple: take your training data, remove the target, assemble your training data together with your test data, and create a new binary classification target where the positive label is assigned to the test data. At this point, run a machine learning classifier and evaluate for the ROC-AUC evaluation metric (we discussed this metric in the previous chapter on Detailing Competition Tasks and Metrics).\n",
        "\n",
        "If your ROC-AUC is around 0.5, it means that the training and test data are not easily distinguishable and are apparently from the same distribution. ROC-AUC values higher than 0.5 and nearing 1.0 signal that it is easy for the algorithm to figure out what is from the training set and what is from the test set: in such a case, don’t expect to be able to easily generalize to the test set because it clearly comes from a different distribution.\n",
        "https://www.kaggle.com/code/konradb/adversarial-validation-and-other-scary-terms/notebook"
      ],
      "metadata": {
        "id": "O4ZrjSAl0p3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all sub folders under directory\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
      ],
      "metadata": {
        "id": "2hGW4cAPxrWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most of the commands in the code you see on Kaggle Notebooks, you will find a parameter declaring a number, a seed, as the random state. This setting is important for the reproducibility of your results. Since many algorithms are not deterministic but are based on randomness, by setting a seed you influence the behavior of the random generator, making it predictable in its randomness: the same random seed corresponds to the same sequence of random numbers. In other words, it allows you to obtain the same results after every run of the same code.Again, reproducibility can be used to your advantage when dealing with public Notebooks. Most often, these Notebooks will have a fixed seed that could be 0, 1, or 42. The value 42 is quite popular because it is a reference to Douglas Adam’s The Hitchhiker’s Guide to the Galaxy, in which it is the “Answer to the Ultimate Question of Life, the Universe, and Everything,” calculated by an enormous supercomputer named Deep Thought over a period of 7.5 million years. Now, if everyone in a competition is using the same random seed, it could have a double effect:"
      ],
      "metadata": {
        "id": "uzSb-YUiHwbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "In an EDA for a Kaggle competition, you will be looking for:\n",
        "\n",
        "Missing values and, most importantly, missing value patterns correlated with the target.\n",
        "Skewed numeric variables and their possible transformations.\n",
        "Rare categories in categorical variables that can be grouped together.\n",
        "Potential outliers, both univariate and multivariate.\n",
        "Highly correlated (or even duplicated) features. For categorical variables, focus on categories that overlap.\n",
        "The most predictive features for the problem."
      ],
      "metadata": {
        "id": "IYydd6qlJYGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, as a piece of advice based on our experience, don’t expect a neural network to be the best model in a tabular competition; this seldom happens. Instead, blend solutions from classical tabular data models, such as gradient boosting models and neural networks, because they tend to pick up different signals from the data that you can integrate together in an ensemble."
      ],
      "metadata": {
        "id": "oVEP8iQdaE4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear models\n",
        "The linear models that need to be tuned are usually linear regressions or logistic regressions with regularization:\n",
        "\n",
        "C: The range you should search is np.logspace(-4, 4, 10); smaller values specify stronger regularization.\n",
        "alpha: You should search the range np.logspace(-2, 2, 10); smaller values specify stronger regularization, larger values specify stronger regularization. Also take note that higher values take more time to process when using lasso.\n",
        "l1_ratio: You should pick from the list [.1, .5, .7, .9, .95, .99, 1]; it applies only to elastic net."
      ],
      "metadata": {
        "id": "nfYzFA40fu5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While XGBoost uses decision trees to split on a variable and explore different tree splits at that variable (the level-wise tree growth strategy), LightGBM concentrates on one split and goes on splitting from there in order to achieve a better fit (the leaf-wise tree growth strategy). Algorithmically speaking, if we think of the structure of splits operated by a decision tree as a graph, XGBoost pursues a breadth-first search (BFS) and LightGBM a depth-first search (DFS).\n",
        "\n",
        "If you are ensembling models with performances that are too different, you will soon find out that there is no point because the net effect will be negative: as you are not smoothing your incorrect predictions, you are also degrading the correct ones."
      ],
      "metadata": {
        "id": "KIkmo-cAfxQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the name of highest paid person (including benefits)?\n",
        "df[df['TotalPayBenefits']== df['TotalPayBenefits'].max()] #['EmployeeName']\n",
        "# or\n",
        "# df.loc[sal['TotalPayBenefits'].idxmax()\n",
        "df[df['TotalPayBenefits'] == df[\"TotalPayBenefits\"].max()]['TotalPayBenefits']"
      ],
      "metadata": {
        "id": "QpfqcBTD_w70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Directly put the log path in the logdir param\n",
        "%tensorboard --logdir=\"vision_model/Xception/20220522-120512/\""
      ],
      "metadata": {
        "id": "HrG37L28L8i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the DataFrame with the missingno library:\n",
        "import missingno as msno\n",
        "\n",
        "msno.matrix(df, figsize=(10, 5), labels=True);"
      ],
      "metadata": {
        "id": "AfHHVD0kXGWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Target Encoding\n",
        "If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (the probability of your target conditional on each category value). It may appear a simple and smart feature engineering trick but it has side effects, mostly in terms of overfitting, because you are taking information from the target into your predictors."
      ],
      "metadata": {
        "id": "MuVcvb7zuAmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for imbalanced datasets use imblearn library\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state= 0)\n",
        "df_review_bal,df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],df_review_imb['sentiment'])\n",
        "\n",
        "\n",
        "df_review_bal"
      ],
      "metadata": {
        "id": "B4jo77Rqt-8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display data with space and format.\n",
        "df_apple = pd.read_csv('../input/google-apple-facebook-stock-price/AAPL.csv')\n",
        "df_google = pd.read_csv('../input/google-apple-facebook-stock-price/GOOG.csv')\n",
        "df_fb = pd.read_csv('../input/google-apple-facebook-stock-price/META.csv')\n",
        "\n",
        "print(clr.S+'Apple Stock Prices\\n------------------'+clr.E)\n",
        "display(df_apple.head())\n",
        "print(clr.S+'Google Stock Prices\\n-------------------'+clr.E)\n",
        "display(df_google.head())\n",
        "print(clr.S+'Facebook Stock Prices\\n---------------------'+clr.E)\n",
        "display(df_fb.head())"
      ],
      "metadata": {
        "id": "nYgjdZ4nuCYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zi97fhaWpzO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It will guarantee that, no matter what, discounted prices calculated by this function cannot be lower than $0 and they cannot be higher than the original price\n",
        "\n",
        "def apply_discount(product, discount):\n",
        "price = int(product['price'] * (1.0 - discount))\n",
        "assert 0 <= price <= product['price']\n",
        "return price"
      ],
      "metadata": {
        "id": "LmSGhUz3p06Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display progress bar\n",
        "from time import sleep\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for i in tqdm(list(range(100))):\n",
        "  sleep(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "6f3446088b4246febba3d93c40adafdf",
            "94de8b9042d14ce591d8a22d2ccfdf5b",
            "65a69944eb6d4f5783180c01273883df",
            "9bc8ea3812504ed2987563905c7379b8",
            "93357a2e808b49618a905d2c32318b82",
            "b0b550ef78c141db873fa3dfeac8d728",
            "9f72112b52734aba84604c04df39f0a0",
            "e33e959b7ddb4b0d9e648768d6fa8e6e",
            "eb3aef622f0043eba9aae3a33060e655",
            "37a1364b405142e693514dba5699818f",
            "551225791e994ee4a847fc868b085901"
          ]
        },
        "id": "CeGFR0O5V9Zj",
        "outputId": "130ee5ce-b65d-4758-ace1-8add55fb182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f3446088b4246febba3d93c40adafdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca1694fdd702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does AUC says about out model?\n",
        "Suppose you get AUC=0.85 when you build a model to detect deseace from xray images.This means that if you select a random image from positive sample and another random image from negative samples, the image from positive sample will rank higher than image from negative sample with a probability of 0.85\n"
      ],
      "metadata": {
        "id": "QAaZNvDe3Omf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multicolinearity\n",
        "If input feature is highly correlated with one or more other features, they are said to be multiollinear, Multicollinearity could be detrimental to the performance of a linear regression model based on least quares. Lets suppose we use two features, x1 and x2 to predict the target variable y. In a linear regression model, we are essentially estimating weights for each of the \n",
        "features that will help predict the target variable such taht the suqred error is minimized. Using least squares, the weight for feature x1 or the effect of x1 on target variable y is estimated by holding a x2 constant. Similary the weight for x2 is estimated by holding 1 constant.If x1 and x2 are collinear theny they vary togethr and it cbecomes very difficult to accuratedly esstimate their effects on the target variable. One of the features becomes completely redundant for the model. \n"
      ],
      "metadata": {
        "id": "F8YR7tufjHND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the ℓ2 norm, noted ∥ · ∥2 (or just ∥ · ∥).\n",
        "\n",
        "Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n",
        "\n",
        "More generally, the ℓk norm of a vector v containing n elements is defined as ∥v∥k = (|v1|k + |v2|k + ... + |vn|k)1/k. ℓ0 gives the number of nonzero elements in the vector, and ℓ∞ gives the maximum absolute value in the vector.\n",
        "\n",
        "The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred."
      ],
      "metadata": {
        "id": "RhMulDbnp0kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places where there is a high density of data points"
      ],
      "metadata": {
        "id": "23W1SWapvEcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing.plot(kind='scatter',x='longitude',y='latitude',grid=True,alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4nr6B6hwvGFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how much each attribute correlates with median house price\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "apwkdu-5wpRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sparse matrix is a very efficient representation for matrices that contain mostly zeros. Indeed, internally it only stores the nonzero values and their positions. When a categorical attribute has hundreds or thousands of categories, one-hot encoding it results in a very large matrix full of 0s except for a single 1 per row. In this case, a sparse matrix is exactly what you need: it will save plenty of memory and speed up computations. You can use a sparse matrix mostly like a normal 2D array,⁠12 but if you want to convert it to a (dense) NumPy array, just call the toarray() method:"
      ],
      "metadata": {
        "id": "lc7NSmnMb8G9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with all estimators, it is important to fit the scalers to the training data only: never use fit() or fit_transform() for anything else than the training set. Once you have a trained scaler, you can then use it to transform() any other set, including the validation set, the test set, and new data. Note that while the training set values will always be scaled to the specified range, if new data contains outliers, these may end up scaled outside the range. If you want to avoid this, just set the clip hyperparameter to True."
      ],
      "metadata": {
        "id": "cHW0wNE9ds84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 (e.g., neural networks work best with zero-mean inputs, so a range of –1 to 1 is preferable). It’s quite easy to use:"
      ],
      "metadata": {
        "id": "xrbMLOqbd446"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INstead of writing big query in one line, separate iit in parts so that its easy to understand\n",
        "df_agg=df.groupby(['Grouping','Year'])['Time']\n",
        "         .min()\n",
        "         .reset_index()\n",
        "         .fillna(0)\n",
        "         .sort_values('Year')"
      ],
      "metadata": {
        "id": "7raeGPCJt9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The .filter method allows columns to be searched with regular expressions using the regex parameter. Here, we search for all columns that have a digit somewhere in their name:\n",
        "movies.filter(regex=r\"\\d\").head()"
      ],
      "metadata": {
        "id": "7RFWYquY4Jme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# While using describe, transpose results . It that way it can fit more data in the window and data is easy to explore\n",
        "movies.describe().T"
      ],
      "metadata": {
        "id": "nJJd_Gwm5Z0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chanining methods in pandas\n",
        "movies.isnull().sum().sum()# total nans in all dataset."
      ],
      "metadata": {
        "id": "QUtMH9lP5zov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JavaScript Object Notation (JSON) is a common format used for transferring data over the internet. Contrary to the name, it does not require JavaScript to read or create. The Python standard library ships with the json library that will encode and decode from JSON:"
      ],
      "metadata": {
        "id": "uTcfHOgb9hAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = https://en.wikipedia.org/wiki/The_Beatles_discography\n",
        "dfs = pd.read_html(url)\n",
        "len(dfs)"
      ],
      "metadata": {
        "id": "yxn4ld71-IYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the memory usage of each column with the .memory_usage method\n",
        "original_mem = col2.memory_usage(deep=True)"
      ],
      "metadata": {
        "id": "eUWCkzqgIyXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object columns are targets for the largest memory savings. pandas has an additional categorical data type that is not available in NumPy. When converting to category, pandas internally creates a mapping from integers to each unique string value. Thus, each string only needs to be kept a single time in memory. As you can see, this change of data type reduced memory usage by 97%."
      ],
      "metadata": {
        "id": "cP5zHSeRJcI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the .nlargest method to select the top 100 movies by imdb_score\n",
        "movie2.nlargest(100, \"imdb_score\").head()"
      ],
      "metadata": {
        "id": "-UPOmxdMJ6nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#is possible to sort one column in ascending order while simultaneously sorting another column in descending order. To accomplish this, pass in a list of Booleans to the ascending parameter that corresponds to how you would like each column sorted. The following sorts title_year and content_rating in descending order and budget in ascending order. It then finds the lowest budget film for each year and content rating group\n",
        "(\n",
        "...     movie[\n",
        "...         [\n",
        "...             \"movie_title\",\n",
        "...             \"title_year\",\n",
        "...             \"content_rating\",\n",
        "...             \"budget\",\n",
        "...         ]\n",
        "...     ]\n",
        "...     .sort_values(\n",
        "...         [\"title_year\", \"content_rating\", \"budget\"],\n",
        "...         ascending=[False, False, True],\n",
        "...     )\n",
        "...     .drop_duplicates(\n",
        "...         subset=[\"title_year\", \"content_rating\"]\n",
        "...     )\n",
        "... )"
      ],
      "metadata": {
        "id": "YcV---uNKe9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Kolmogorov-Smirnov test can evaluate whether a distribution is normal. It provides us with a p-value. If this value is significant (< 0.05), then the data is not normal:\n",
        "from scipy import stats\n",
        "stats.kstest(fueleco.city08, cdf=\"norm\")\n",
        "KstestResult(statistic=0.9999999990134123, pvalue=0.0)"
      ],
      "metadata": {
        "id": "fz0lQTi76u-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson correlation is intended to show the strength of a linear relationship. If the two continuous columns do not have a linear relationship, another option is to use Spearman correlation. This number also varies from -1 to 1. It measures whether the relationship is monotonic (and doesn't presume that it is linear). It uses the rank of each number rather than the number. If you are not sure whether there is a linear relationship between your columns, this is a better metric to use."
      ],
      "metadata": {
        "id": "TAHS3ua-FDXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fueleco.city08.corr(\n",
        "fueleco.barrels08, method=\"spearman\")"
      ],
      "metadata": {
        "id": "8J8xowG5FN6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a third-party library, pandas Profiling (https://pandas-profiling.github.io/pandas-profiling/docs/), that creates reports for each column. These reports are similar to the output of the .describe method, but include plots and other descriptive statistics."
      ],
      "metadata": {
        "id": "45F0y-KnITgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-profiling\n",
        "import pandas_profiling as pp\n",
        "pp.ProfileReport(fueleco)"
      ],
      "metadata": {
        "id": "8YfASxh7IReb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get first four row and, first column using both loc and iloc\n",
        "college.iloc[[0, 4], 0] \n",
        "college.loc[\n",
        "            \"Alabama A & M University\",\n",
        "            \"Alabama State University\",\n",
        "            ],\n",
        "               \"CITY\",\n",
        "                ]\n",
        "#Care needs to be taken when using slicing off of .loc. If the start index appears after the stop index, then an empty Series is returned without an exception:"
      ],
      "metadata": {
        "id": "V68URaEChd47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " To select rows and columns, you will need to pass both valid row and column selections separated by a comma to either .iloc or .loc.\n",
        "\n"
      ],
      "metadata": {
        "id": "upr9sOZ1iqfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The generic form to select rows and columns will look like the following code:\n",
        "df.iloc[row_idxs, column_idxs]\n",
        "df.loc[row_names, column_names]"
      ],
      "metadata": {
        "id": "w-VTwn8wi0Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Use of query\n",
        " top10_depts = (\n",
        "    employee.DEPARTMENT.value_counts()\n",
        "    .index[:10]\n",
        "    .tolist()\n",
        "    )\n",
        "qs = \"DEPARTMENT not in @top10_depts and GENDER == 'Female'\"\n",
        "employee_filtered2 = employee.query(qs)\n",
        "employee_filtered2.head()"
      ],
      "metadata": {
        "id": "Z_mNpy4djyof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Where\n",
        "criteria_high = fb_likes < 20_000\n",
        "criteria_low = fb_likes > 300\n",
        "\n",
        "fb_likes_cap = fb_likes.where(\n",
        " criteria_high, other=20_000\n",
        ").where(criteria_low, 300)\n",
        "fb_likes_cap.head()"
      ],
      "metadata": {
        "id": "biwvenh0kSeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['list', 'of', 'grouping', 'columns'])\n",
        "df.groupby('single_column')  # when grouping by a single column "
      ],
      "metadata": {
        "id": "oPHcUbgLlgKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .stack method takes all of the column names and pivots them into the index. Typically, when you call the .stack method, the data becomes taller."
      ],
      "metadata": {
        "id": "fC0e8JdSTibS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(state_fruit\n",
        "...    .stack()\n",
        "...    .reset_index()\n",
        "...    .rename(columns={'level_0':'state', \n",
        "...       'level_1': 'fruit', 0: 'weight'})\n",
        "... )"
      ],
      "metadata": {
        "id": "s-IlGJ3tTyc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "concat:\n",
        "\n",
        "A pandas function\n",
        "Combines two or more pandas objects vertically or horizontally\n",
        "Aligns only on the index\n",
        "Errors whenever a duplicate appears in the index\n",
        "Defaults to outer join with the option for inner join\n",
        "\n",
        "join:\n",
        "\n",
        "A DataFrame method\n",
        "Combines two or more pandas objects horizontally\n",
        "Aligns the calling DataFrame's column(s) or index with the other object's index (and not the columns)\n",
        "Handles duplicate values on the joining columns/index by performing a Cartesian product\n",
        "Defaults to left join with options for inner, outer, and right\n",
        "\n",
        "merge:\n",
        "\n",
        "A DataFrame method\n",
        "Combines exactly two DataFrames horizontally\n",
        "Aligns the calling DataFrame's column(s) or index with the other DataFrame's column(s) or index\n",
        "Handles duplicate values on the joining columns or index by performing a cartesian product\n",
        "Defaults to inner join with options for left, outer, and right"
      ],
      "metadata": {
        "id": "CMoI9_YXTxR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime\n",
        "...     [crime.REPORTED_DATE.between(\n",
        "...          '2015-3-4','2016-1-1 23:59:59')]\n",
        "...     .shape"
      ],
      "metadata": {
        "id": "jvgk0MPvMWMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))    \n",
        ">>> (employee\n",
        "...     .groupby('RACE', sort=False) \n",
        "...     ['BASE_SALARY']\n",
        "...     .mean()\n",
        "...     .plot.barh(rot=0, width=.8, ax=ax)\n",
        "... )\n",
        ">>> ax.set_xlabel('Mean Salary')"
      ],
      "metadata": {
        "id": "KLzUFi9QS_NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jupyter, however has some tricks up its sleeves. If you tack on a single question mark (?) following a function or method, it will show the documentation for that code. Note that this is not valid Python syntax, it is a feature of Jupyter. If you add on two question marks (??), then Jupyter will display the source code of the function or method."
      ],
      "metadata": {
        "id": "vrs9HrseEEdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you are going to perform any data preprocessing steps (for example, missing value imputation, feature engineering, standardization, label encoding, and more), you have to build the function based on the train set and then apply it to the validation and test set. Do not perform those data preprocessing steps on the full original data (before data splitting). That's because it might lead to a data leakage problem."
      ],
      "metadata": {
        "id": "hHkUrhmaTffA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a pop-culture reference! In Douglas Adams's popular 1979 science-fiction novel The Hitchhiker's Guide to the Galaxy, towards the end of the book, the supercomputer Deep Thought reveals that the answer to the great question of “life, the universe and everything” is 42."
      ],
      "metadata": {
        "id": "6Jw8cmXZVPv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " There are several reasons why we need to apply the cross-validation strategy:\n",
        "\n",
        "Having only a small amount of training data.\n",
        "To get a more confident conclusion from the evaluation performance.\n",
        "To get a clearer picture of our model's learning ability and/or the complexity of the given data."
      ],
      "metadata": {
        "id": "T6KwhTiiVuJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeated k-fold cross-validation involves simply performing the k-fold cross-validation repeatedly, N times, with different randomizations in each repetition. The final evaluation score is the average of all scores from all folds of each repetition. This strategy will increase our confidence in our model.\n",
        "\n",
        "So, why repeat the k-fold cross-validation? Why don't we just increase the value of k in k-fold? Surely, increasing the value of k will reduce the bias of our model's estimated performance. However, increasing the value of k will increase the variation, especially when we have a small number of samples. Therefore, usually, repeating the k-folds is a better way to gain higher confidence in our model's estimated performance. Of course, this comes with a drawback, which is the increase in computation time."
      ],
      "metadata": {
        "id": "580Fmz4OgAHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "df_cv, df_test = train_test_split(df, test_size=0.2, random_state=0, stratify=df['class'])\n",
        "rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=0)\n",
        "for train_index, val_index in rskf.split(df_cv, df_cv['class']):\n",
        "df_train, df_val = df_cv.iloc[train_index], df_cv.iloc[val_index]\n",
        "#perform training or hyperparameter tuning here"
      ],
      "metadata": {
        "id": "sDe8aKd1gRm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leave One Out (LOO) cross-validation is just k-fold cross-validation where k = n, where n is the number of samples. This means there are n-1 samples for the training set and 1 sample for the validation set in each fold "
      ],
      "metadata": {
        "id": "QhqQkBF8gu0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, when is LOO preferred over k-fold cross-validation? Well, LOO works best when you have a very small dataset. It is also good to choose LOO over k-fold if you prefer the high confidence of the model's performance estimation over the computational cost limitation."
      ],
      "metadata": {
        "id": "UBBAzfnEgywh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time-series data has a unique characteristic in nature. Unlike \"normal\" data, which is assumed to be independent and identically distributed (IID), time-series data does not follow that assumption. In fact, each sample is dependent on previous samples, meaning changing the order of the samples will result in different data interpretations"
      ],
      "metadata": {
        "id": "5fV08oolhUEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "df_cv, df_test = train_test_split(df, test_size=0.2, random_state=0, shuffle=False)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for train_index, val_index in tscv.split(df_cv):\n",
        "df_train, df_val = df_cv.iloc[train_index], df_cv.iloc[val_index]\n",
        "#perform training or hyperparameter tuning here"
      ],
      "metadata": {
        "id": "1-mjIki7kQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of hyperparameter tuning is simply to get the maximum evaluation score on the validation set without causing an overfitting issue.Hyperparameter tuning is one of the model-centric approaches to optimizing a model's performance. In practice, it is suggested to prioritize data-centric approaches over a model-centric approach when it comes to optimizing a model's performance. Data-centric means that we are focusing on cleaning, sampling, augmenting, or modifying the data, while model-centric means that we are focusing on the model and its configuration."
      ],
      "metadata": {
        "id": "F2vIYgc5kO4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between a hyperparameter and a parameter is how its value is generated. A parameter value is generated by the model during the model-training phase. In other words, its value is learned from the given data instead of given by the developer. On the other hand, a hyperparameter value is given by the developer since it can't be estimated from the data.\n",
        "Parameters: Weights in NN and coefficient in regression.\n",
        "Hyperparameters: epochs, batch size, dropout rate, number of layers, number of neurons"
      ],
      "metadata": {
        "id": "m85JZighlTIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You also need to be aware that there are models without hyperparameters or parameters, but not both of them. For instance, a linear regression model is a model that has only parameters but doesn't have any hyperparameters. On the other hand, K-Nearest Neighbors (KNN) is an instance of a model that doesn't contain any parameters but has a k hyperparameter."
      ],
      "metadata": {
        "id": "SsYwSP6amF3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The COD in Table 3.2 means that adding another value to the hyperparameter space will exponentially increase the experiment time. Let's use the preceding example where we performed hyperparameter tuning on a random forest. In our initial hyperparameter space, there are  combinations we have to test. If we add just another value to our space—let's say we add 30 to the max_depth list—there will be  combinations or an additional 10 combinations that we have to test. "
      ],
      "metadata": {
        "id": "J8zAjKrmoaz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian optimization (BO) is the second out of four groups of hyperparameter tuning methods. Unlike grid search and random search, which are categorized as uninformed search methods, all of the methods that belong to the BO group are categorized as informed search methods, meaning they are learning from previous iterations to (hopefully) provide a better search space in the future."
      ],
      "metadata": {
        "id": "3WOV_ezZptGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a hyperparameter tuning method, you are recommended to utilize a GA(Genetic Algoriths) when each trial doesn’t take too much time and you have enough computational resources, such as parallel computing resources."
      ],
      "metadata": {
        "id": "Td6wMvIGKnhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn also provides a Pipeline object that can be used along with the hyperparameter tuning classes. This Pipeline object will ensure that any data preprocessing steps are only fitted based on the train set during the cross-validation. Essentially, this object is just a chain of several sklearn transformers and estimators, which has the same fit() and predict() method, just like a usual sklearn estimator."
      ],
      "metadata": {
        "id": "s-BYfH0NcDBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = GridSearchCV(model, hyperparameter_space,\n",
        "\n",
        "                   scoring='f1', cv=5,\n",
        "\n",
        "                   n_jobs=-1, refit = True)"
      ],
      "metadata": {
        "id": "SP66wZ2HeW1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The n_jobs parameter controls how many jobs you want to run in parallel. If you decide to use all of the processors, you can simply set n_jobs=–1, just as we did in the example.\n",
        "\n",
        "Last but not least, we have the refit parameter. This Boolean parameter is responsible for deciding whether at the end of the hyperparameter tuning process we want to refit our model on the full train set using the best set of hyperparameters or not. In this example, we set refit=True, meaning that sklearn will automatically refit our RF model on the full train set using the best set of hyperparameters. It is very important to retrain our model on the full train set after performing hyperparameter tuning since we only utilize subsets of the train set during the hyperparameter tuning process."
      ],
      "metadata": {
        "id": "_wrxSrR7d4we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike Hyperopt, which assumes we are always working with a minimization problem  we can tell Optuna the type of optimization problem we are working on: minimization or maximization."
      ],
      "metadata": {
        "id": "Cv54NyPWd5bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree can be utilized to perform a classification or regression task by constructing a series of decisions (in the form of rules and splitting points) that can be visualized in the form of a tree. These decisions are made by looking through all of the features and the feature values of the given training data. The goal of a decision tree is to have high homogeneity for each of the leaf nodes. Several methods can be used to measure homogeneity; the two most popular methods for classification tasks are to calculate the Gini or Entropy values, while the most popular method for regression tasks is to calculate the Mean Squared Error value."
      ],
      "metadata": {
        "id": "roOsA9sR5Ho0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_features: This specifies the number of randomly sampled features that are used by Random Forest to choose the best splitting point in each of the decision trees. The higher the value, the lower the reduction in variance, and hence the lower the increase in bias. A higher value also leads to having a longer computational time. scikit-learn, by default, will use all of the features for regression tasks and use only sqrt(n_features) number of features for classification tasks."
      ],
      "metadata": {
        "id": "AvpzxHx05q3I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DN-F7aCaEDMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean encoding transforms categorical columns into numerical columns based on the mean target variable. For instance, if the color orange led to seven target values of 1 and three target values of 0, the mean encoded column would be 7/10 = 0.7. Since there is data leakage while using the target values, additional regularization techniques are required.\n",
        "\n",
        "Data leakage occurs when information between training and test sets, or predictor and target columns, are shared. The risk here is that the target column is being directly used to influence the predictor columns, which is generally a bad idea in machine learning. Nevertheless, mean encoding has been shown to produce outstanding results. It can work when datasets are deep, and the distribution of mean values are approximately the same for incoming data. Regularization is an extra precaution taken to reduce the possibility of overfitting."
      ],
      "metadata": {
        "id": "Qyq7CAQv51Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade category_encoders\n",
        "\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "encoder = TargetEncoder()"
      ],
      "metadata": {
        "id": "Kkj7gxYx56df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally, introduce a new column and apply mean encoding using the fit_transform method on the encoder. Include the column that is being changed and the target column as parameters\n",
        "df['cab_type_mean'] = encoder.fit_transform(df['cab_type'], df['price'])"
      ],
      "metadata": {
        "id": "OVcK1FoS76UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering is an essential skill for any data scientist to build robust models. The strategies covered here are only a fraction of the options that exist. Feature engineering involves research, experimentation, domain expertise, standardizing columns, feedback on the machine learning performance of new columns, and narrowing down the final columns at the end."
      ],
      "metadata": {
        "id": "ZqbiIM7K7W7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For missing values mputation, median is often a better choice than the mean. The median guarantees that half the data is greater than the given value and half the data is lower. The mean, by contrast, is vulnerable to outliers."
      ],
      "metadata": {
        "id": "kj_6EjA3GwcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's common for users to make mistakes with single or double brackets when using pandas. .iloc uses single brackets for one index as follows: df_bikes.iloc[56]. Now, df_bikes also accepts a list inside brackets to allow multiple indices. Multiple indices require double brackets as follows: df_bikes.iloc[[56, 81]]."
      ],
      "metadata": {
        "id": "9KP1Fg5rHK-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find correlation in data\n",
        "\n",
        "pearson_corr_series = pd.Series(df.corr().iloc[-1,:-1], index=df.columns[:-1])\n",
        "plt.figure(figsize=(28,1))\n",
        "sns.heatmap([pearson_corr_series], annot=True, cmap='Blues', xticklabels=pearson_corr_series.index)"
      ],
      "metadata": {
        "id": "WJk9p1NCiTBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot bar chart of correlation features\n",
        "abs(pearson_corr_series).sort_values(ascending=0).plot.bar()\n",
        "#Top most co related feature pairs\n",
        "df.corr().abs().unstack().sort_values(ascending=0)[9::2]"
      ],
      "metadata": {
        "id": "TqIOaC1tibkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mean encodings with different regularisations were tested on 6 datasets along with one-hot, label and frequency encodings. Datasets mostly consisted of high-cardinality categorical features.\n",
        "\n",
        "k-fold regularisation using 4 or 5 folds and  α=5  should be your \"to go\" regularisation for mean encoding, it almost always shows good results.\n",
        "\n",
        "Most of the time, there is no point to even try to use mean encoding without a prior ( α ). It always performs worst then the one with a prior.\n",
        "\n",
        "Expanding mean regularisation works well only on bigger datasets (>100000 samples).\n",
        "\n",
        "Frequency encoding works surprisingly good on many datasets, I suggest you to always give it a try.\n",
        "\n",
        "Mean encodings let models converge faster (in terms of a number of iterations) then the frequency and label encodings.\n",
        "\n",
        "Performance of a particular encoding depends a lot on a dataset you work with. Any encoding can strongly outperform all other encodings for a number of different reasons."
      ],
      "metadata": {
        "id": "wZzaTttuXEv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trend(columns):\n",
        "    for column in columns:\n",
        "        movies = df_eda[df_eda['item_category_name'] == column]\n",
        "        movies = movies.groupby('date_block_num')['item_cnt_day'].sum().to_frame().reset_index()\n",
        "        plt.figure(figsize=(20,4))\n",
        "        #Draw a horizontal reference line \n",
        "        plt.axhline (y =movies['item_cnt_day'].mean(), color='red', linewidth = 1, linestyle ='dashed', label = 'Tüm zamanlar için ortalama satışlar.')\n",
        "        sns.lineplot(data=movies, x='date_block_num', y='item_cnt_day', label = 'Trend line.')\n",
        "        plt.title('Satın alma eğilimi ' + str(column) + '.', fontsize=14)\n",
        "        plt.legend(loc = 'upper right')\n",
        "        plt.xlabel('Ay Sayısı (Ocak 2013ten Ekim 2015e kadar)')\n",
        "        plt.ylabel('Satılan ürünler')\n",
        "        plt.show()\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "zuHkeeMdr96k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downcast(df, verbose=True):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        dtype_name = df[col].dtype.name\n",
        "        if dtype_name == 'object':\n",
        "            pass\n",
        "        elif dtype_name == 'bool':\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif dtype_name.startswith('int') or (df[col].round() == df[col]).all():\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "        else:\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print('{:.1f}% compressed'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "all_df = [sales_train, shops, items, item_categories, test]\n",
        "for df in all_df:\n",
        "    df = downcast(df)"
      ],
      "metadata": {
        "id": "_jLijGbVfte6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features with a lot of different categories require different approaches to encoding then low cardinality features. One-hot encoding will create a huge amount of columns and harm the column sampling process in tree methods: one-hot encoded columns will overcrowd all other predictors and make high cardinality feature to be way too disproportionally important for a model. Label encoding will be hard to process for a model, because of how random it will look and how many splits will be needed."
      ],
      "metadata": {
        "id": "O2BLzg56puSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n"
      ],
      "metadata": {
        "id": "_N74XTpmpr_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why scoring='neg_mean_squared_error'? Scikit-learn is designed to select the highest score when training models. This works well for accuracy, but not for errors when the lowest is best. By taking the negative of each mean squared error, the lowest ends up being the highest. This is compensated for later with rmse = np.sqrt(-scores), so the final results are positive."
      ],
      "metadata": {
        "id": "TolrY4Z9Rk-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pd.get_dummies may increase memory usage, as can be verified using the .info() method on the DataFrame in question and checking the last line. Sparse matrices may be used to save memory where only values of 1 are stored and values of 0 are not stored."
      ],
      "metadata": {
        "id": "T8PqMPlESvru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second line of the root reads gini=0.364. This is the error method the decision tree uses to decide how splits should be made. The goal is to find a split that leads to the lowest error. A Gini index of 0 means 0 errors. A gini index of 1 means all errors. A gini index of 0.5, which shows an equal distribution of elements, means the predictions are no better than random guessing. The closer to 0, the lower the error. At the root, a gini of 0.364 means the training set is imbalanced with 36.4 percent of class 1."
      ],
      "metadata": {
        "id": "CYyfGSMgjCrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally speaking, decreasing max hyperparameters and increasing min hyperparameters will reduce variation and prevent overfitting."
      ],
      "metadata": {
        "id": "y_RykB3dIweI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7HikZjnpIu7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gini index of 0 means 0 errors. A gini index of 1 means all errors. A gini index of 0.5, which shows an equal distribution of elements, means the predictions are no better than random guessing. The closer to 0, the lower the error. At the root, a gini of 0.364 means the training set is imbalanced with 36.4 percent of class 1."
      ],
      "metadata": {
        "id": "HSZxU6sTfFVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A straight line generally has high bias. In machine learning bias is a mathematical term that comes from estimating the error when applying the model to a real-life problem. The bias of the straight line is high because the predictions are restricted to the line and fail to account for changes in the data."
      ],
      "metadata": {
        "id": "bOzempGUf2Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some data scientists, feature engineering is how we narrow down the features needed for supervised models (e.g., trying to predict a response or outcome variable). For others, it is the methodology used to extract numerical representations from unstructured data for an unsupervised model (e.g., trying to extract structure from a previously unstructured dataset). Feature engineering is both of these and much more."
      ],
      "metadata": {
        "id": "Pu6KeFMvzP1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also more powerful imputers available in the sklearn.impute package (both for numerical features only):\n",
        "\n",
        "KNNImputer replaces each missing value with the mean of the k-nearest neighbors’ values for that feature. The distance is based on all the available features.\n",
        "\n",
        "IterativeImputer trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration."
      ],
      "metadata": {
        "id": "ew8BLOzTHREo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, you can use one of the encoders provided by the category_encoders package on GitHub. Or, when dealing with neural networks, you can replace each category with a learnable, low-dimensional vector called an embedding."
      ],
      "metadata": {
        "id": "yM7XiLr2HMiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Feature improvement—Making existing features more usable through mathematical transformations\n",
        "\n",
        "Example—Imputing (filling in) missing temperatures on a weather dataset by inferring them from the other columns\n",
        "\n",
        "Feature construction—Augmenting the dataset by creating new interpretable features from existing interpretable features\n",
        "\n",
        "Example—Dividing the total price of home feature by the square foot of home feature to create a price per square foot feature in a home-valuation dataset\n",
        "\n",
        "Feature selection—Choosing the best subset of features from an existing set of features\n",
        "\n",
        "Example—After creating the price per square foot feature, possibly removing the previous two features if they don’t add any value to the ML model anymore\n",
        "\n",
        "Feature extraction—Relying on algorithms to automatically create new, sometimes uninterpretable, features, usually based on making parametric assumptions about the data\n",
        "\n",
        "Example—Relying on pretrained transfer learning models, like Google’s BERT, to map unstructured text to a structured and generally uninterpretable vector space\n",
        "\n",
        "Feature learning—Automatically generating a brand new set of features, usually by extracting structure and learning representations from raw unstructured data, such as text, images, and videos, often using deep learning\n",
        "\n",
        "Example—Training generative adversarial networks (GANs) to deconstruct and reconstruct images for the purposes of learning the optimal representation for a given task"
      ],
      "metadata": {
        "id": "PizqLxgX7FEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While min-max standardization ensures that each feature is on the same scale (from 0 to 1), z-score standardization ensures that outliers are handled more properly but will not guarantee that the data will end up on the exact same scale.\n",
        "Both transformations do not affect the distribution of the feature like the log and Box-Cox transformations, and they both help deal with the effects of outliers on our models. Min-max standardization has a harder time dealing with outliers, so if our data have many outliers, it is generally better to stick with z-score standardization."
      ],
      "metadata": {
        "id": "dMnsH_f6Op7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vet_visits.drop_duplicates(subset=['name','breed'])# drops duplicate on name and breed\n",
        "unique_dogs['breed'].value_counts(sort=True)# sort values in descending order\n",
        "unique_dogs['breed'].value_counts(normalize=True)# Shows proportion instead of counts\n",
        "dogs.groupby('color')['weights_kg'].agg([mean,max,sum])# Groupby aand agg \n",
        "dogs.pivot_table(values='weights_kg',index='color',aggfun=['np.mean','np.median'])#same as above but using pivot table\n",
        "dogs.pivot_table(values='weights_kg',index='color',columns='breed',fill_value=0,margins=True)#more than one column and NaN should be 0 and also columnwise and rowwise sum.\n",
        "# Index temperatures by country & city\n",
        "temperatures_ind = temperatures.set_index(['country','city'])\n",
        "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
        "rows_to_keep = [('Brazil','Rio De Janeiro'),('Pakistan','Lahore')]\n",
        "# Subset for rows to keep\n",
        "print(temperatures_ind.loc[rows_to_keep])\n",
        "# Get 23rd row, 2nd column (index 22, 1)\n",
        "print(temperatures.iloc[22, 1])\n",
        "# Use slicing to get the first 5 rows\n",
        "print(temperatures.iloc[:5])\n",
        "# Use slicing to get columns 3 to 4\n",
        "print(temperatures.iloc[:, 2:4])\n",
        "# Use slicing in both directions at once\n",
        "print(temperatures.iloc[:5, 2:4])\n",
        "dogs.isna().sum().plot(kind=bar)\n",
        "# Check individual values for missing values\n",
        "print(avocados_2016.isna())\n",
        "# Check each column for missing values\n",
        "print(avocados_2016.isna().any())\n",
        "# Bar plot of missing values by variable\n",
        "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
        "# Use melt on ten_yr, unpivot everything besides the metric column\n",
        "bond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close')\n",
        "# Use query on bond_perc to select only the rows where metric=close\n",
        "bond_perc_close = bond_perc.query('metric == \"close\"')\n",
        "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
        "dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', \n",
        "                            suffixes=('_dow', '_bond'), how='inner')\n",
        "# Plot only the close_dow and close_bond columns\n",
        "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
        "plt.show()\n",
        "# Import date\n",
        "from datetime import date\n",
        "\n",
        "# Create a date object\n",
        "andrew = date(1992, 8, 26)\n",
        "\n",
        "# Print the date in the format 'YYYY-DDD'\n",
        "print(andrew.strftime('%Y-%j'))\n",
        "print(andrew.strftime('%Y-%m-%d %H-%M-%S'))\n",
        "# Count the number of missing values in each column\n",
        "print(planes.isna().sum())\n",
        "\n",
        "# Find the five percent threshold\n",
        "threshold = len(planes) * 0.05\n",
        "\n",
        "# Create a filter\n",
        "cols_to_drop = planes.columns[planes.isna().sum() <= threshold]\n",
        "\n",
        "# Drop missing values for columns below the threshold\n",
        "planes.dropna(subset=cols_to_drop, inplace=True)\n",
        "\n",
        "print(planes.isna().sum())\n",
        "np.var(msleep['sleep'],ddof=1)#ddof means degrees of freedom if we dont mention this then it will calculate population variance instead of sample variance"
      ],
      "metadata": {
        "id": "tagnYvhCzJo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}