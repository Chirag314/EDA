{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ai4code.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag314/EDA/blob/main/ai4code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thanks @thedevastator for your great [work](https://www.kaggle.com/code/thedevastator/codebert-pairwise)\n",
        "\n",
        "_____\n",
        "**Credits:**<br>\n",
        "This notebook demonstrates a simple ensemble method for ranking problems. It is **based on the two incredible notebooks:**\n",
        "- **[Stronger baseline with code cells](https://www.kaggle.com/code/suicaokhoailang/stronger-baseline-with-code-cells)** by [suicaokhoailang](https://www.kaggle.com/suicaokhoailang)\n",
        "- **[AI4Code Pairwise BertSmall inference](https://www.kaggle.com/code/yuanzhezhou/ai4code-pairwise-bertsmall-inference)** by [yuanzhezhou](https://www.kaggle.com/yuanzhezhou)<br>\n",
        "\n",
        "All credits for the models themselves (both training and prediction) belogs to the original authors! I simply cloned their code and retrained my own version.\n",
        "_____\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ensembling Rank Based Submissions\n",
        "\n",
        "We are a month away from the finish line and yet there is no high scoring ensemble in sight!\n",
        "\n",
        "Let's fix this. \n",
        "\n",
        "But how do you actually combine rank based predictions? Let alone predictions that come from completly different approaches (direct rank prediction / pairwise).<br>\n",
        "Actually, this is pretty simple: **Average the indices of the elements.**<br>\n",
        "This way, we can sort the final prediction by the ensembled indices and it will simply represent an aggragated representation of the element's location.<br>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018432,
          "end_time": "2022-05-23T03:29:09.581162",
          "exception": false,
          "start_time": "2022-05-23T03:29:09.56273",
          "status": "completed"
        },
        "tags": [],
        "id": "Yp3bLmMIxpAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "\n",
        "### **[Stronger baseline with code cells](https://www.kaggle.com/code/suicaokhoailang/stronger-baseline-with-code-cells)**\n",
        "#### By [suicaokhoailang](https://www.kaggle.com/suicaokhoailang)"
      ],
      "metadata": {
        "id": "JknHUyz0xpAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_notebook(path):\n",
        "    import pandas as pd\n",
        "    return (\n",
        "        pd.read_json(\n",
        "            path,\n",
        "            dtype={'cell_type': 'category', 'source': 'str'})\n",
        "        .assign(id=path.stem)\n",
        "        .rename_axis('cell_id')\n",
        "    )"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.114595,
          "end_time": "2022-05-23T03:29:09.832611",
          "exception": false,
          "start_time": "2022-05-23T03:29:09.718016",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:09.556553Z",
          "iopub.execute_input": "2022-07-30T16:21:09.556812Z",
          "iopub.status.idle": "2022-07-30T16:21:09.58414Z",
          "shell.execute_reply.started": "2022-07-30T16:21:09.556737Z",
          "shell.execute_reply": "2022-07-30T16:21:09.583495Z"
        },
        "trusted": true,
        "id": "gaRNruCmxpAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_code(cell): return str(cell).replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "def sample_cells(cells, n):\n",
        "    import numpy as np\n",
        "    cells = [clean_code(cell) for cell in cells]\n",
        "    if n >= len(cells): return [cell[:200] for cell in cells]\n",
        "    else:\n",
        "        results = []\n",
        "        step = len(cells) / n\n",
        "        idx = 0\n",
        "        while int(np.round(idx)) < len(cells):\n",
        "            results.append(cells[int(np.round(idx))])\n",
        "            idx += step        \n",
        "        if cells[-1] not in results: results[-1] = cells[-1]\n",
        "        return results\n",
        "\n",
        "def get_features(df):\n",
        "    from tqdm import tqdm\n",
        "    features = dict()\n",
        "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
        "    for idx, sub_df in tqdm(df.groupby(\"id\")):\n",
        "        features[idx] = dict()\n",
        "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
        "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
        "        total_code = code_sub_df.shape[0]\n",
        "        codes = sample_cells(code_sub_df.source.values, 20)\n",
        "        features[idx][\"total_code\"] = total_code\n",
        "        features[idx][\"total_md\"] = total_md\n",
        "        features[idx][\"codes\"] = codes\n",
        "    return features"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.023767,
          "end_time": "2022-05-23T03:29:09.929422",
          "exception": false,
          "start_time": "2022-05-23T03:29:09.905655",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:09.602267Z",
          "iopub.execute_input": "2022-07-30T16:21:09.602474Z",
          "iopub.status.idle": "2022-07-30T16:21:09.612742Z",
          "shell.execute_reply.started": "2022-07-30T16:21:09.602449Z",
          "shell.execute_reply": "2022-07-30T16:21:09.611888Z"
        },
        "trusted": true,
        "id": "cvp52BjaxpAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_sc(document):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    return document\n",
        "\n",
        "def preprocess_text_sc1(document):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "#     return document\n",
        "    #Lemmatization\n",
        "    tokens = document.split()\n",
        "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "    # tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:09.641767Z",
          "iopub.execute_input": "2022-07-30T16:21:09.642169Z",
          "iopub.status.idle": "2022-07-30T16:21:30.966777Z",
          "shell.execute_reply.started": "2022-07-30T16:21:09.642139Z",
          "shell.execute_reply": "2022-07-30T16:21:30.965858Z"
        },
        "trusted": true,
        "id": "wgUGEw9TxpAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data): return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
        "\n",
        "def validate(model, val_loader):    \n",
        "    import sys\n",
        "    import torch    \n",
        "    import numpy as np\n",
        "    from tqdm import tqdm    \n",
        "    model.eval()    \n",
        "    tbar = tqdm(val_loader, file=sys.stdout)    \n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(tbar):\n",
        "            inputs, target = read_data(data)\n",
        "            pred = model(*inputs)\n",
        "            preds.append(pred.detach().cpu().numpy().ravel())\n",
        "            labels.append(target.detach().cpu().numpy().ravel())    \n",
        "    return np.concatenate(labels), np.concatenate(preds)\n",
        "\n",
        "def predict_caller(args): return predict(args[0], args[1],args[2])\n",
        "    \n",
        "def predict(model_path, ckpt_path, output_file):\n",
        "    \n",
        "    import gc\n",
        "    import json\n",
        "    import sys, os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from pathlib import Path\n",
        "    from scipy import sparse\n",
        "\n",
        "    data_dir = Path('../input/AI4Code')\n",
        "    paths_test = list((data_dir / 'test').glob('*.json'))\n",
        "    notebooks_test = [\n",
        "        read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
        "    ]\n",
        "    test_df = (\n",
        "        pd.concat(notebooks_test)\n",
        "        .set_index('id', append=True)\n",
        "        .swaplevel()\n",
        "        .sort_index(level='id', sort_remaining=False)\n",
        "    ).reset_index()\n",
        "    test_df[\"rank\"] = test_df.groupby([\"id\", \"cell_type\"]).cumcount()\n",
        "    test_df[\"pred\"] = test_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import DataLoader, Dataset\n",
        "    from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "    class MarkdownModel(nn.Module):\n",
        "        def __init__(self, model_path):\n",
        "            super(MarkdownModel, self).__init__()\n",
        "            self.model = AutoModel.from_pretrained(model_path)\n",
        "            self.top = nn.Linear(769, 1)\n",
        "\n",
        "        def forward(self, ids, mask, fts):\n",
        "            x = self.model(ids, mask)[0]\n",
        "            x = self.top(torch.cat((x[:, 0, :], fts),1))\n",
        "            return x\n",
        "\n",
        "\n",
        "    class MarkdownDataset(Dataset):\n",
        "\n",
        "        def __init__(self, df, model_name_or_path, total_max_len, md_max_len, fts):\n",
        "            super().__init__()\n",
        "            self.df = df.reset_index(drop=True)\n",
        "            self.md_max_len = md_max_len\n",
        "            self.total_max_len = total_max_len  # maxlen allowed by model config\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "            self.fts = fts\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            row = self.df.iloc[index]\n",
        "\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                row.source,\n",
        "                None,\n",
        "                add_special_tokens=True,\n",
        "                max_length=self.md_max_len,\n",
        "                padding=\"max_length\",\n",
        "                return_token_type_ids=True,\n",
        "                truncation=True\n",
        "            )\n",
        "            code_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [str(x) for x in self.fts[row.id][\"codes\"]],\n",
        "                add_special_tokens=True,\n",
        "                max_length=23,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True\n",
        "            )\n",
        "            n_md = self.fts[row.id][\"total_md\"]\n",
        "            n_code = self.fts[row.id][\"total_md\"]\n",
        "            if n_md + n_code == 0:\n",
        "                fts = torch.FloatTensor([0])\n",
        "            else:\n",
        "                fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
        "\n",
        "            ids = inputs['input_ids']\n",
        "            for x in code_inputs['input_ids']:\n",
        "                ids.extend(x[:-1])\n",
        "            ids = ids[:self.total_max_len]\n",
        "            if len(ids) != self.total_max_len:\n",
        "                ids = ids + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(ids))\n",
        "            ids = torch.LongTensor(ids)\n",
        "\n",
        "            mask = inputs['attention_mask']\n",
        "            for x in code_inputs['attention_mask']:\n",
        "                mask.extend(x[:-1])\n",
        "            mask = mask[:self.total_max_len]\n",
        "            if len(mask) != self.total_max_len:\n",
        "                mask = mask + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(mask))\n",
        "            mask = torch.LongTensor(mask)\n",
        "\n",
        "            assert len(ids) == self.total_max_len\n",
        "\n",
        "            return ids, mask, fts, torch.FloatTensor([row.pct_rank])\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.df.shape[0]\n",
        "    \n",
        "    model = MarkdownModel(model_path)\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    BS = 32\n",
        "    NW = 8\n",
        "    MAX_LEN = 64\n",
        "    test_df[\"pct_rank\"] = 0\n",
        "    test_fts = get_features(test_df)\n",
        "    test_md = test_df[test_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)\n",
        "    test_md.source = test_md.source.apply(preprocess_text_sc)\n",
        "\n",
        "    test_ds = MarkdownDataset(test_md, md_max_len=64,total_max_len=512, model_name_or_path=model_path, fts=test_fts)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
        "                              pin_memory=False, drop_last=False)\n",
        "    _, y_test = validate(model, test_loader)\n",
        "    model.to(torch.device('cpu'))\n",
        "    torch.cuda.empty_cache()    \n",
        "    del model, test_loader, test_ds\n",
        "    gc.collect()      \n",
        "    \n",
        "    test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_test\n",
        "    sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "    sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
        "    sub_df.head()\n",
        "    sub_df.to_csv(output_file, index=False)\n",
        "\n",
        "    del test_df, paths_test, notebooks_test, test_fts, model_path, ckpt_path, sub_df\n",
        "    del json, np, pd, tqdm, Path, sparse, torch, sys, os, nn, F, AutoModel, AutoTokenizer\n",
        "    gc.collect()\n",
        "    "
      ],
      "metadata": {
        "papermill": {
          "duration": 0.027037,
          "end_time": "2022-05-23T03:29:16.100816",
          "exception": false,
          "start_time": "2022-05-23T03:29:16.073779",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:30.96881Z",
          "iopub.execute_input": "2022-07-30T16:21:30.969073Z",
          "iopub.status.idle": "2022-07-30T16:21:31.001628Z",
          "shell.execute_reply.started": "2022-07-30T16:21:30.969036Z",
          "shell.execute_reply": "2022-07-30T16:21:31.000905Z"
        },
        "trusted": true,
        "id": "3LKoR5tkxpAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caller1(args): return predict1(args[0], args[1],  args[2],  args[3])\n",
        "    \n",
        "def predict1(model_path, ckpt_path, output_file, pre_process = False):\n",
        "    \n",
        "    import gc\n",
        "    import json\n",
        "    import sys, os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from pathlib import Path\n",
        "    from scipy import sparse\n",
        "\n",
        "    data_dir = Path('../input/AI4Code')\n",
        "    paths_test = list((data_dir / 'test').glob('*.json'))\n",
        "    notebooks_test = [\n",
        "        read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
        "    ]\n",
        "    test_df = (\n",
        "        pd.concat(notebooks_test)\n",
        "        .set_index('id', append=True)\n",
        "        .swaplevel()\n",
        "        .sort_index(level='id', sort_remaining=False)\n",
        "    ).reset_index()\n",
        "    test_df[\"rank\"] = test_df.groupby([\"id\", \"cell_type\"]).cumcount()\n",
        "    test_df[\"pred\"] = test_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import DataLoader, Dataset\n",
        "    from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "    class MarkdownModel(nn.Module):\n",
        "        def __init__(self, model_path):\n",
        "            super(MarkdownModel, self).__init__()\n",
        "            self.model = AutoModel.from_pretrained(model_path)\n",
        "            self.top = nn.Linear(769, 1)\n",
        "\n",
        "        def forward(self, ids, mask, fts):\n",
        "            x = self.model(ids, mask)[0]\n",
        "            x = self.top(torch.cat((x[:, 0, :], fts),1))\n",
        "            return x\n",
        "\n",
        "\n",
        "    class MarkdownDataset(Dataset):\n",
        "\n",
        "        def __init__(self, df, model_name_or_path, total_max_len, md_max_len, fts):\n",
        "            super().__init__()\n",
        "            self.df = df.reset_index(drop=True)\n",
        "            self.md_max_len = md_max_len\n",
        "            self.total_max_len = total_max_len  # maxlen allowed by model config\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "            self.fts = fts\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            row = self.df.iloc[index]\n",
        "\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                row.source,\n",
        "                None,\n",
        "                add_special_tokens=True,\n",
        "                max_length=self.md_max_len,\n",
        "                padding=\"max_length\",\n",
        "                return_token_type_ids=True,\n",
        "                truncation=True\n",
        "            )\n",
        "            code_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [str(x) for x in self.fts[row.id][\"codes\"]],\n",
        "                add_special_tokens=True,\n",
        "                max_length=23,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True\n",
        "            )\n",
        "            n_md = self.fts[row.id][\"total_md\"]\n",
        "            n_code = self.fts[row.id][\"total_md\"]\n",
        "            if n_md + n_code == 0:\n",
        "                fts = torch.FloatTensor([0])\n",
        "            else:\n",
        "                fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
        "\n",
        "            ids = inputs['input_ids']\n",
        "            for x in code_inputs['input_ids']:\n",
        "                ids.extend(x[:-1])\n",
        "            ids = ids[:self.total_max_len]\n",
        "            if len(ids) != self.total_max_len:\n",
        "                ids = ids + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(ids))\n",
        "            ids = torch.LongTensor(ids)\n",
        "\n",
        "            mask = inputs['attention_mask']\n",
        "            for x in code_inputs['attention_mask']:\n",
        "                mask.extend(x[:-1])\n",
        "            mask = mask[:self.total_max_len]\n",
        "            if len(mask) != self.total_max_len:\n",
        "                mask = mask + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(mask))\n",
        "            mask = torch.LongTensor(mask)\n",
        "\n",
        "            assert len(ids) == self.total_max_len\n",
        "\n",
        "            return ids, mask, fts, torch.FloatTensor([row.pct_rank])\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.df.shape[0]\n",
        "    \n",
        "    model = MarkdownModel(model_path)\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    BS = 32\n",
        "    NW = 8\n",
        "    MAX_LEN = 64\n",
        "    test_df[\"pct_rank\"] = 0\n",
        "    test_fts = get_features(test_df)\n",
        "    test_md = test_df[test_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)\n",
        "    if pre_process:\n",
        "        test_md.source = test_md.source.apply(preprocess_text_sc1)\n",
        "    test_ds = MarkdownDataset(test_md, md_max_len=64,total_max_len=512, model_name_or_path=model_path, fts=test_fts)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
        "                              pin_memory=False, drop_last=False)\n",
        "    _, y_test = validate(model, test_loader)\n",
        "    model.to(torch.device('cpu'))\n",
        "    torch.cuda.empty_cache()    \n",
        "    del model, test_loader, test_ds\n",
        "    gc.collect()      \n",
        "    \n",
        "    test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_test\n",
        "    sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "    sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
        "    sub_df.head()\n",
        "#     sub_df.to_csv(\"submission_1.csv\", index=False)\n",
        "    sub_df.to_csv(output_file, index = False)\n",
        "\n",
        "    del test_df, paths_test, notebooks_test, test_fts, model_path, ckpt_path, sub_df\n",
        "    del json, np, pd, tqdm, Path, sparse, torch, sys, os, nn, F, AutoModel, AutoTokenizer\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:31.003123Z",
          "iopub.execute_input": "2022-07-30T16:21:31.003602Z",
          "iopub.status.idle": "2022-07-30T16:21:31.034547Z",
          "shell.execute_reply.started": "2022-07-30T16:21:31.003567Z",
          "shell.execute_reply": "2022-07-30T16:21:31.033891Z"
        },
        "trusted": true,
        "id": "yoOLDljrxpAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "ckpt_path1 = \"../input/ai4code-model/model.bin\"\n",
        "ckpt_path3 = \"../input/codebert-v3-epoch4/model.bin\"\n",
        "ckpt_path2 = \"../input/codebert-v2-epoch5/model.bin\"\n",
        "model_path = \"../input/codebert-base/codebert-base/\"\n",
        "output_file1 = \"submission_1.csv\"\n",
        "output_file2 = \"submission_3.csv\"\n",
        "output_file3 = \"submission_4.csv\"\n",
        "\n",
        "from tqdm.contrib.concurrent import process_map\n",
        "process_map(predict_caller1, [(model_path, ckpt_path1, output_file1, False)])[0]\n",
        "process_map(predict_caller1, [(model_path, ckpt_path3, output_file3, True)])[0]\n",
        "process_map(predict_caller, [(model_path, ckpt_path2, output_file2)])[0]\n",
        "gc.collect()"
      ],
      "metadata": {
        "papermill": {
          "duration": 20.374059,
          "end_time": "2022-05-23T03:29:36.523092",
          "exception": false,
          "start_time": "2022-05-23T03:29:16.149033",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-07-30T16:21:31.037017Z",
          "iopub.execute_input": "2022-07-30T16:21:31.037506Z",
          "iopub.status.idle": "2022-07-30T16:22:35.080086Z",
          "shell.execute_reply.started": "2022-07-30T16:21:31.037469Z",
          "shell.execute_reply": "2022-07-30T16:22:35.079281Z"
        },
        "trusted": true,
        "id": "yQdmgSXyxpAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "\n",
        "### **[AI4Code Pairwise BertSmall inference](https://www.kaggle.com/code/yuanzhezhou/ai4code-pairwise-bertsmall-inference)**\n",
        "#### By [yuanzhezhou](https://www.kaggle.com/yuanzhezhou)"
      ],
      "metadata": {
        "id": "9EJhxrgSxpAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "\n",
        "pd.options.display.width = 180\n",
        "pd.options.display.max_colwidth = 120\n",
        "\n",
        "BERT_PATH = \"../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased\"\n",
        "\n",
        "data_dir = Path('../input/AI4Code')\n",
        "NUM_TRAIN = 200\n",
        "\n",
        "def read_notebook(path):\n",
        "    return (\n",
        "        pd.read_json(\n",
        "            path,\n",
        "            dtype={'cell_type': 'category', 'source': 'str'})\n",
        "        .assign(id=path.stem)\n",
        "        .rename_axis('cell_id')\n",
        "    )\n",
        "\n",
        "paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n",
        "notebooks_train = [\n",
        "    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
        "]\n",
        "df = (\n",
        "    pd.concat(notebooks_train)\n",
        "    .set_index('id', append=True)\n",
        "    .swaplevel()\n",
        "    .sort_index(level='id', sort_remaining=False)\n",
        ")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017111,
          "end_time": "2022-05-23T03:29:36.734096",
          "exception": false,
          "start_time": "2022-05-23T03:29:36.716985",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:35.081383Z",
          "iopub.execute_input": "2022-07-30T16:22:35.081646Z",
          "iopub.status.idle": "2022-07-30T16:22:39.67732Z",
          "shell.execute_reply.started": "2022-07-30T16:22:35.081606Z",
          "shell.execute_reply": "2022-07-30T16:22:39.676602Z"
        },
        "trusted": true,
        "id": "D9FYdasLxpAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an example notebook\n",
        "nb_id = df.index.unique('id')[6]\n",
        "print('Notebook:', nb_id)\n",
        "\n",
        "print(\"The disordered notebook:\")\n",
        "nb = df.loc[nb_id, :]\n",
        "display(nb)\n",
        "print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:39.679802Z",
          "iopub.execute_input": "2022-07-30T16:22:39.680306Z",
          "iopub.status.idle": "2022-07-30T16:22:39.695389Z",
          "shell.execute_reply.started": "2022-07-30T16:22:39.680261Z",
          "shell.execute_reply": "2022-07-30T16:22:39.694696Z"
        },
        "trusted": true,
        "id": "9l7PK_ARxpAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_orders = pd.read_csv(\n",
        "    data_dir / 'train_orders.csv',\n",
        "    index_col='id',\n",
        "    squeeze=True,\n",
        ").str.split()  # Split the string representation of cell_ids into a list\n",
        "\n",
        "df_orders"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:39.69662Z",
          "iopub.execute_input": "2022-07-30T16:22:39.696869Z",
          "iopub.status.idle": "2022-07-30T16:22:42.257119Z",
          "shell.execute_reply.started": "2022-07-30T16:22:39.696834Z",
          "shell.execute_reply": "2022-07-30T16:22:42.256307Z"
        },
        "trusted": true,
        "id": "myg-cjsAxpAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_orders.loc[\"002ba502bdac45\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.258611Z",
          "iopub.execute_input": "2022-07-30T16:22:42.258878Z",
          "iopub.status.idle": "2022-07-30T16:22:42.266126Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.258841Z",
          "shell.execute_reply": "2022-07-30T16:22:42.265232Z"
        },
        "trusted": true,
        "id": "q1EzPEo7xpAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_order = df_orders.loc[nb_id]\n",
        "\n",
        "print(\"The ordered notebook:\")\n",
        "nb.loc[cell_order, :]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.267472Z",
          "iopub.execute_input": "2022-07-30T16:22:42.267901Z",
          "iopub.status.idle": "2022-07-30T16:22:42.284601Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.267862Z",
          "shell.execute_reply": "2022-07-30T16:22:42.28369Z"
        },
        "trusted": true,
        "id": "Sl0bgk4AxpAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ranks(base, derived):\n",
        "    return [base.index(d) for d in derived]\n",
        "\n",
        "cell_ranks = get_ranks(cell_order, list(nb.index))\n",
        "nb.insert(0, 'rank', cell_ranks)\n",
        "\n",
        "nb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.289065Z",
          "iopub.execute_input": "2022-07-30T16:22:42.28931Z",
          "iopub.status.idle": "2022-07-30T16:22:42.303539Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.289281Z",
          "shell.execute_reply": "2022-07-30T16:22:42.302734Z"
        },
        "trusted": true,
        "id": "ebqBQYMZxpAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orders_ = df_orders.to_frame().join(\n",
        "    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
        "    how='right',\n",
        ")\n",
        "\n",
        "ranks = {}\n",
        "for id_, cell_order, cell_id in df_orders_.itertuples():\n",
        "    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
        "\n",
        "df_ranks = (\n",
        "    pd.DataFrame\n",
        "    .from_dict(ranks, orient='index')\n",
        "    .rename_axis('id')\n",
        "    .apply(pd.Series.explode)\n",
        "    .set_index('cell_id', append=True)\n",
        ")\n",
        "\n",
        "df_ranks"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.304749Z",
          "iopub.execute_input": "2022-07-30T16:22:42.305045Z",
          "iopub.status.idle": "2022-07-30T16:22:42.387528Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.30501Z",
          "shell.execute_reply": "2022-07-30T16:22:42.386735Z"
        },
        "trusted": true,
        "id": "dN09ipsSxpAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
        "df_ancestors"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.38891Z",
          "iopub.execute_input": "2022-07-30T16:22:42.389162Z",
          "iopub.status.idle": "2022-07-30T16:22:42.587229Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.389127Z",
          "shell.execute_reply": "2022-07-30T16:22:42.586581Z"
        },
        "trusted": true,
        "id": "oJDEKpqpxpAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.588589Z",
          "iopub.execute_input": "2022-07-30T16:22:42.588831Z",
          "iopub.status.idle": "2022-07-30T16:22:42.655308Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.588796Z",
          "shell.execute_reply": "2022-07-30T16:22:42.654611Z"
        },
        "trusted": true,
        "id": "lc-zyUR_xpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n",
        "df[\"pct_rank\"].hist(bins=10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.65648Z",
          "iopub.execute_input": "2022-07-30T16:22:42.656804Z",
          "iopub.status.idle": "2022-07-30T16:22:42.950934Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.656752Z",
          "shell.execute_reply": "2022-07-30T16:22:42.949972Z"
        },
        "trusted": true,
        "id": "j8WIpDBkxpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_cellid_source = dict(zip(df['cell_id'].values, df['source'].values))\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk; nltk.download('wordnet')\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(document):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "        #return document\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "        tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "        return preprocessed_text\n",
        "\n",
        "    \n",
        "def preprocess_df(df):\n",
        "    \"\"\"\n",
        "    This function is for processing sorce of notebook\n",
        "    returns preprocessed dataframe\n",
        "    \"\"\"\n",
        "    return [preprocess_text(message) for message in df.source]\n",
        "\n",
        "df.source = df.source.apply(preprocess_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:22:42.952854Z",
          "iopub.execute_input": "2022-07-30T16:22:42.953194Z",
          "iopub.status.idle": "2022-07-30T16:23:07.101668Z",
          "shell.execute_reply.started": "2022-07-30T16:22:42.953146Z",
          "shell.execute_reply": "2022-07-30T16:23:07.100899Z"
        },
        "trusted": true,
        "id": "HHcSYtIgxpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "NVALID = 0.1  # size of validation set\n",
        "\n",
        "splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n",
        "\n",
        "train_ind, val_ind = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n",
        "\n",
        "train_df = df.loc[train_ind].reset_index(drop=True)\n",
        "val_df = df.loc[val_ind].reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:07.102957Z",
          "iopub.execute_input": "2022-07-30T16:23:07.103304Z",
          "iopub.status.idle": "2022-07-30T16:23:07.123282Z",
          "shell.execute_reply.started": "2022-07-30T16:23:07.103265Z",
          "shell.execute_reply": "2022-07-30T16:23:07.122524Z"
        },
        "trusted": true,
        "id": "9DB7rKXRxpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def generate_triplet(df, mode='train'):\n",
        "  triplets = []\n",
        "  ids = df.id.unique()\n",
        "  random_drop = np.random.random(size=10000)>0.9\n",
        "  count = 0\n",
        "\n",
        "  for id, df_tmp in tqdm(df.groupby('id')):\n",
        "    df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown']\n",
        "\n",
        "    df_tmp_code = df_tmp[df_tmp['cell_type']=='code']\n",
        "    df_tmp_code_rank = df_tmp_code['rank'].values\n",
        "    df_tmp_code_cell_id = df_tmp_code['cell_id'].values\n",
        "\n",
        "    for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values:\n",
        "      labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int')\n",
        "\n",
        "      for cid, label in zip(df_tmp_code_cell_id, labels):\n",
        "        count += 1\n",
        "        if label==1:\n",
        "          triplets.append( [cell_id, cid, label] )\n",
        "          # triplets.append( [cid, cell_id, label] )\n",
        "        elif mode == 'test':\n",
        "          triplets.append( [cell_id, cid, label] )\n",
        "          # triplets.append( [cid, cell_id, label] )\n",
        "        elif random_drop[count%10000]:\n",
        "          triplets.append( [cell_id, cid, label] )\n",
        "          # triplets.append( [cid, cell_id, label] )\n",
        "    \n",
        "  return triplets\n",
        "\n",
        "triplets = generate_triplet(train_df)\n",
        "val_triplets = generate_triplet(val_df, mode = 'test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:07.124578Z",
          "iopub.execute_input": "2022-07-30T16:23:07.124988Z",
          "iopub.status.idle": "2022-07-30T16:23:07.658465Z",
          "shell.execute_reply.started": "2022-07-30T16:23:07.12495Z",
          "shell.execute_reply": "2022-07-30T16:23:07.657645Z"
        },
        "trusted": true,
        "id": "pkPOJ9f1xpAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:07.659828Z",
          "iopub.execute_input": "2022-07-30T16:23:07.660186Z",
          "iopub.status.idle": "2022-07-30T16:23:07.678388Z",
          "shell.execute_reply.started": "2022-07-30T16:23:07.660147Z",
          "shell.execute_reply": "2022-07-30T16:23:07.677771Z"
        },
        "trusted": true,
        "id": "LvZiPkuBxpAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bisect import bisect\n",
        "\n",
        "\n",
        "def count_inversions(a):\n",
        "    inversions = 0\n",
        "    sorted_so_far = []\n",
        "    for i, u in enumerate(a):\n",
        "        j = bisect(sorted_so_far, u)\n",
        "        inversions += i - j\n",
        "        sorted_so_far.insert(j, u)\n",
        "    return inversions\n",
        "\n",
        "\n",
        "def kendall_tau(ground_truth, predictions):\n",
        "    total_inversions = 0\n",
        "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
        "    for gt, pred in zip(ground_truth, predictions):\n",
        "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
        "        total_inversions += count_inversions(ranks)\n",
        "        n = len(gt)\n",
        "        total_2max += n * (n - 1)\n",
        "    return 1 - 4 * total_inversions / total_2max"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:07.679655Z",
          "iopub.execute_input": "2022-07-30T16:23:07.679901Z",
          "iopub.status.idle": "2022-07-30T16:23:07.687805Z",
          "shell.execute_reply.started": "2022-07-30T16:23:07.679867Z",
          "shell.execute_reply": "2022-07-30T16:23:07.686838Z"
        },
        "trusted": true,
        "id": "ms3HL4kGxpAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "    \n",
        "class MarkdownModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MarkdownModel, self).__init__()\n",
        "        self.distill_bert = AutoModel.from_pretrained(\"../input/mymodelpairbertsmallpretrained/models/checkpoint-18000\")\n",
        "        self.top = nn.Linear(512, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, ids, mask):\n",
        "        x = self.distill_bert(ids, mask)[0]\n",
        "        x = self.dropout(x)\n",
        "        x = self.top(x[:, 0, :])\n",
        "        x = torch.sigmoid(x) \n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:07.689064Z",
          "iopub.execute_input": "2022-07-30T16:23:07.689596Z",
          "iopub.status.idle": "2022-07-30T16:23:09.720905Z",
          "shell.execute_reply.started": "2022-07-30T16:23:07.689555Z",
          "shell.execute_reply": "2022-07-30T16:23:09.720102Z"
        },
        "trusted": true,
        "id": "jZvUqLwdxpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "\n",
        "class MarkdownDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, max_len, mode='train'):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"../input/mymodelpairbertsmallpretrained/my_own_tokenizer\", do_lower_case=True)\n",
        "        self.mode=mode\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df[index]\n",
        "\n",
        "        label = row[-1]\n",
        "\n",
        "        txt = dict_cellid_source[row[0]] + '[SEP]' + dict_cellid_source[row[1]]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            txt,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = torch.LongTensor(inputs['input_ids'])\n",
        "        mask = torch.LongTensor(inputs['attention_mask'])\n",
        "\n",
        "        return ids, mask, torch.FloatTensor([label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.722449Z",
          "iopub.execute_input": "2022-07-30T16:23:09.722719Z",
          "iopub.status.idle": "2022-07-30T16:23:09.732795Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.722682Z",
          "shell.execute_reply": "2022-07-30T16:23:09.732077Z"
        },
        "trusted": true,
        "id": "i9KkU2lUxpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_lr(optimizer, epoch):\n",
        "    if epoch < 1:\n",
        "        lr = 5e-5\n",
        "    elif epoch < 2:\n",
        "        lr = 1e-3\n",
        "    elif epoch < 5:\n",
        "        lr = 1e-4\n",
        "    else:\n",
        "        lr = 1e-5\n",
        "\n",
        "    for p in optimizer.param_groups:\n",
        "        p['lr'] = lr\n",
        "    return lr\n",
        "    \n",
        "def get_optimizer(net):\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999), eps=1e-08)\n",
        "    return optimizer\n",
        "\n",
        "BS = 128\n",
        "NW = 8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.733839Z",
          "iopub.execute_input": "2022-07-30T16:23:09.734596Z",
          "iopub.status.idle": "2022-07-30T16:23:09.74448Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.734556Z",
          "shell.execute_reply": "2022-07-30T16:23:09.743795Z"
        },
        "trusted": true,
        "id": "XHPmNh4vxpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data):\n",
        "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
        "\n",
        "def validate(model, val_loader, mode='train'):\n",
        "    model.eval()\n",
        "    \n",
        "    tbar = tqdm(val_loader, file=sys.stdout)\n",
        "    \n",
        "    preds = np.zeros(len(val_loader.dataset), dtype='float32')\n",
        "    labels = []\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(tbar):\n",
        "            inputs, target = read_data(data)\n",
        "\n",
        "            pred = model(inputs[0], inputs[1]).detach().cpu().numpy().ravel()\n",
        "\n",
        "            preds[count:count+len(pred)] = pred\n",
        "            count += len(pred)\n",
        "            \n",
        "            if mode=='test':\n",
        "              labels.append(target.detach().cpu().numpy().ravel())\n",
        "    if mode=='test':\n",
        "      return preds\n",
        "    else:\n",
        "      return np.concatenate(labels), np.concatenate(preds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.74732Z",
          "iopub.execute_input": "2022-07-30T16:23:09.74751Z",
          "iopub.status.idle": "2022-07-30T16:23:09.75956Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.747486Z",
          "shell.execute_reply": "2022-07-30T16:23:09.758894Z"
        },
        "trusted": true,
        "id": "Jh7LOwGgxpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths_test = list((data_dir / 'test').glob('*.json'))\n",
        "notebooks_test = [\n",
        "    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
        "]\n",
        "test_df = (\n",
        "    pd.concat(notebooks_test)\n",
        "    .set_index('id', append=True)\n",
        "    .swaplevel()\n",
        "    .sort_index(level='id', sort_remaining=False)\n",
        ").reset_index()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.762356Z",
          "iopub.execute_input": "2022-07-30T16:23:09.762696Z",
          "iopub.status.idle": "2022-07-30T16:23:09.835753Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.762667Z",
          "shell.execute_reply": "2022-07-30T16:23:09.835107Z"
        },
        "trusted": true,
        "id": "excED9IgxpAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.source = test_df.source.apply(preprocess_text)\n",
        "dict_cellid_source = dict(zip(test_df['cell_id'].values, test_df['source'].values))\n",
        "test_df[\"rank\"] = test_df.groupby([\"id\", \"cell_type\"]).cumcount()\n",
        "test_df[\"pred\"] = test_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=False)\n",
        "test_triplets = generate_triplet(test_df, mode = 'test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.836921Z",
          "iopub.execute_input": "2022-07-30T16:23:09.83732Z",
          "iopub.status.idle": "2022-07-30T16:23:09.93359Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.837281Z",
          "shell.execute_reply": "2022-07-30T16:23:09.932936Z"
        },
        "trusted": true,
        "id": "n0hdycK7xpAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"pct_rank\"] = 0\n",
        "test_ds = MarkdownDataset(test_triplets, max_len=MAX_LEN)\n",
        "test_loader = DataLoader(test_ds, batch_size=BS * 4, shuffle=False, num_workers=NW, pin_memory=False, drop_last=False)\n",
        "\n",
        "import gc \n",
        "gc.collect()\n",
        "len(test_ds), test_ds[0]"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:09.934825Z",
          "iopub.execute_input": "2022-07-30T16:23:09.935279Z",
          "iopub.status.idle": "2022-07-30T16:23:10.336778Z",
          "shell.execute_reply.started": "2022-07-30T16:23:09.935222Z",
          "shell.execute_reply": "2022-07-30T16:23:10.336091Z"
        },
        "trusted": true,
        "id": "_f3ktShexpAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "\n",
        "model = MarkdownModel()\n",
        "model = model.cuda()\n",
        "model.load_state_dict(torch.load('../input/mymodelbertsmallpretrained120000/my_own_model.bin'))\n",
        "y_test = validate(model, test_loader, mode='test')"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:10.338084Z",
          "iopub.execute_input": "2022-07-30T16:23:10.338605Z",
          "iopub.status.idle": "2022-07-30T16:23:18.272595Z",
          "shell.execute_reply.started": "2022-07-30T16:23:10.338563Z",
          "shell.execute_reply": "2022-07-30T16:23:18.271734Z"
        },
        "trusted": true,
        "id": "-VdRPCI5xpAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_copy = y_test\n",
        "pred_vals = []\n",
        "count = 0\n",
        "for id, df_tmp in tqdm(test_df.groupby('id')):\n",
        "  df_tmp_mark = df_tmp[df_tmp['cell_type']=='markdown']\n",
        "  df_tmp_code = df_tmp[df_tmp['cell_type']!='markdown']\n",
        "  df_tmp_code_rank = df_tmp_code['rank'].rank().values\n",
        "  N_code = len(df_tmp_code_rank)\n",
        "  N_mark = len(df_tmp_mark)\n",
        "\n",
        "  preds_tmp = preds_copy[count:count+N_mark * N_code]\n",
        "\n",
        "  count += N_mark * N_code\n",
        "\n",
        "  for i in range(N_mark):\n",
        "    pred = preds_tmp[i*N_code:i*N_code+N_code] \n",
        "\n",
        "    softmax = np.exp((pred-np.mean(pred)) *20)/np.sum(np.exp((pred-np.mean(pred)) *20)) \n",
        "\n",
        "    rank = np.sum(softmax * df_tmp_code_rank)\n",
        "    pred_vals.append(rank)\n",
        "\n",
        "del model\n",
        "del test_triplets[:]\n",
        "del dict_cellid_source\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.277066Z",
          "iopub.execute_input": "2022-07-30T16:23:18.277325Z",
          "iopub.status.idle": "2022-07-30T16:23:18.725575Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.277293Z",
          "shell.execute_reply": "2022-07-30T16:23:18.724737Z"
        },
        "trusted": true,
        "id": "gEfsEoo5xpAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = pred_vals\n",
        "sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
        "sub_df.to_csv(\"submission_2.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.726931Z",
          "iopub.execute_input": "2022-07-30T16:23:18.727268Z",
          "iopub.status.idle": "2022-07-30T16:23:18.74006Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.727215Z",
          "shell.execute_reply": "2022-07-30T16:23:18.73897Z"
        },
        "trusted": true,
        "id": "TzHeO09DxpAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "\n",
        "## Rank Ensemble\n",
        "\n",
        "###### (finally)"
      ],
      "metadata": {
        "id": "s4NA1u3QxpAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now for the moment we have all been waiting for: **Ensemling rank based submissions.**\n",
        "\n",
        "But how are we going to do this?\n",
        "\n",
        "- Let's say that we have two different submissions: \"submission_1.csv\", \"submission_2.csv\". Each containing a list of sorted strings per row.\n",
        "- We would like to create a new submission such that each row contains a sorted list that is an aggregation of the sorted list in the same row of both submissions.\n",
        "- To do this, we simply ensemble the indices. The index is nothing but a rank of a particular string. From the highest likelyhood of the string being in it's expected package to the lowest.\n",
        "- Then sort the strings by their ensembled index."
      ],
      "metadata": {
        "id": "oQAgZbJcxpAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the submissions**"
      ],
      "metadata": {
        "id": "VdcNh4Z7xpAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv('submission_2.csv')\n",
        "df_2 = pd.read_csv('submission_1.csv')\n",
        "df_4 = pd.read_csv('submission_4.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.74174Z",
          "iopub.execute_input": "2022-07-30T16:23:18.742271Z",
          "iopub.status.idle": "2022-07-30T16:23:18.752657Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.742205Z",
          "shell.execute_reply": "2022-07-30T16:23:18.751972Z"
        },
        "trusted": true,
        "id": "Is_gdHJ3xpAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Averaging the indices and sorting the resulting submission by the aggregated ensembled indices**"
      ],
      "metadata": {
        "id": "l5FV88nNxpAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_samples = []\n",
        "for sample_idx in range(len(df_1)):\n",
        "    sample_1 = {k: v for v, k in enumerate(df_1.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "    sample_2 = {k: v for v, k in enumerate(df_2.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "    sample_4 = {k: v for v, k in enumerate(df_4.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "    for key in sample_1: sample_1[key] = ( (sample_1[key] * 0.256) + (sample_2[key] * 0.361) + (sample_4[key] * 0.393))\n",
        "    new_samples.append(' '.join([i[0] for i in list(sorted(sample_1.items(), key=lambda x:x[1]))]))\n",
        "df_1['cell_order'] = new_samples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.754078Z",
          "iopub.execute_input": "2022-07-30T16:23:18.754476Z",
          "iopub.status.idle": "2022-07-30T16:23:18.764398Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.754436Z",
          "shell.execute_reply": "2022-07-30T16:23:18.763639Z"
        },
        "trusted": true,
        "id": "cxb7AN_7xpAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ayR0Nj6SxpAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = pd.read_csv('submission_3.csv')\n",
        "new_samples = []\n",
        "for sample_idx in range(len(df_1)):\n",
        "    sample_1 = {k: v for v, k in enumerate(df_1.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "    sample_2 = {k: v for v, k in enumerate(df_3.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "    for key in sample_1: sample_1[key] = ( (sample_1[key] * 0.49) + (sample_2[key] * 0.51) )\n",
        "    new_samples.append(' '.join([i[0] for i in list(sorted(sample_1.items(), key=lambda x:x[1]))]))\n",
        "df_1['cell_order'] = new_samples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.765575Z",
          "iopub.execute_input": "2022-07-30T16:23:18.765954Z",
          "iopub.status.idle": "2022-07-30T16:23:18.777317Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.76592Z",
          "shell.execute_reply": "2022-07-30T16:23:18.776632Z"
        },
        "trusted": true,
        "id": "aIetD6ZfxpAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_samples = []\n",
        "# for sample_idx in range(len(df_1)):\n",
        "#     sample_1 = {k: v for v, k in enumerate(df_1.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "#     sample_2 = {k: v for v, k in enumerate(df_2.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "#     sample_3 = {k: v for v, k in enumerate(df_2.iloc[sample_idx]['cell_order'].split(' '))}\n",
        "#     for key in sample_1: sample_1[key] = ( (sample_1[key] * 0.251) + (sample_2[key] * 0.749) )\n",
        "#     new_samples.append(' '.join([i[0] for i in list(sorted(sample_1.items(), key=lambda x:x[1]))]))\n",
        "# df_1['cell_order'] = new_samples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T16:23:18.778698Z",
          "iopub.execute_input": "2022-07-30T16:23:18.77895Z",
          "iopub.status.idle": "2022-07-30T16:23:18.785587Z",
          "shell.execute_reply.started": "2022-07-30T16:23:18.778914Z",
          "shell.execute_reply": "2022-07-30T16:23:18.784884Z"
        },
        "trusted": true,
        "id": "dMfy2_BtxpAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T04:06:58.945242Z",
          "iopub.execute_input": "2022-07-30T04:06:58.945495Z",
          "iopub.status.idle": "2022-07-30T04:06:58.95649Z",
          "shell.execute_reply.started": "2022-07-30T04:06:58.94546Z",
          "shell.execute_reply": "2022-07-30T04:06:58.95579Z"
        },
        "trusted": true,
        "id": "0AQKNbx0xpAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving as output so we can submit**"
      ],
      "metadata": {
        "id": "gXMQhdcaxpAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.to_csv('submission.csv', index = False)\n",
        "df_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-30T04:06:58.957689Z",
          "iopub.execute_input": "2022-07-30T04:06:58.957934Z",
          "iopub.status.idle": "2022-07-30T04:06:58.97358Z",
          "shell.execute_reply.started": "2022-07-30T04:06:58.957901Z",
          "shell.execute_reply": "2022-07-30T04:06:58.972944Z"
        },
        "trusted": true,
        "id": "EgtVsE6pxpAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lkqWLf89xpAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}